{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1XOBCh-gyKZcRjkhuVdAjZlAeoBWfCZ47","timestamp":1736534076432},{"file_id":"1zHaOvxWM8twT04dcIaplOJUgz_hCQ1BH","timestamp":1736024230358},{"file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","timestamp":1735856996491}],"gpuType":"A100","mount_file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","authorship_tag":"ABX9TyPy/DYiTnYzmRhJbUqtrKDm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Section 1: Initial Setup and Core Components\n","\n","import os\n","from pathlib import Path\n","import logging\n","import torch\n","import psutil\n","import time\n","import random\n","from datetime import timedelta\n","import sys\n","import io\n","import tempfile\n","import numpy as np\n","from contextlib import contextmanager\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","import warnings\n","from contextlib import nullcontext\n","\n","# Check if running in Google Colab\n","def is_colab():\n","    try:\n","        return 'google.colab' in str(get_ipython())\n","    except NameError:\n","        return False\n","\n","# Mount Google Drive if in Colab\n","if is_colab():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","# Create LLM directory in Drive\n","base_dir = Path('/content/drive/MyDrive/LLM') if is_colab() else Path('./LLM')\n","for dir_name in ['checkpoints', 'models', 'logs', 'configs', 'data']:\n","    (base_dir / dir_name).mkdir(parents=True, exist_ok=True)\n","\n","def is_package_installed(package_name):\n","    try:\n","        __import__(package_name)\n","        return True\n","    except ImportError:\n","        return False\n","\n","if is_colab():\n","    # PyTorch packages\n","    pytorch_packages = ['torch', 'torchvision', 'torchaudio']\n","    pytorch_install = [pkg for pkg in pytorch_packages if not is_package_installed(pkg)]\n","    if pytorch_install:\n","        !pip install {' '.join(pytorch_install)} --index-url https://download.pytorch.org/whl/cu118\n","\n","    # Additional packages\n","    other_packages = ['pynvml', 'nvidia_ml_py3', 'gputil', 'fastapi', 'uvicorn', 'pydantic']\n","    other_install = [pkg for pkg in other_packages if not is_package_installed(pkg)]\n","    if other_install:\n","        !pip install {' '.join(other_install)}\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Section 1: Initial setup and core components complete\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqqmXrJxt-XO","executionInfo":{"status":"ok","timestamp":1736884661617,"user_tz":360,"elapsed":3120,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"128ae982-f083-4f76-bfd7-fddb5eca6a40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: nvidia_ml_py3 in /usr/local/lib/python3.10/dist-packages (7.352.0)\n","Requirement already satisfied: gputil in /usr/local/lib/python3.10/dist-packages (1.4.0)\n","Section 1: Initial setup and core components complete\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp4ZWu0UjQBh"},"outputs":[],"source":["import logging\n","import torch\n","from pathlib import Path\n","from typing import List, Union\n","\n","class DataManager:\n","    def __init__(self, config):\n","        self.config = config\n","        self.char_to_idx = {}\n","        self.idx_to_char = {}\n","        self.train_data: torch.Tensor = None\n","        self.val_data: torch.Tensor = None\n","        self.test_data: torch.Tensor = None\n","        self.vocab_size = 0\n","\n","        # Define the data paths as lists of files\n","        self.data_paths = {\n","            'train': [\n","                Path('/content/drive/MyDrive/LLM/data/huff_2252.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/theo_2253.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/hubbard_2251.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/raekwon_2250.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/pappas_2249.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/trussell_2247.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/fox_2246.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/blagojevich_2245.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/sorin_2242.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/strassman_2241.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/tarantino_2240.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/derek_2239.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/mcphee_2238.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/parks_2236.txt'),\n","            ],\n","            'val': [\n","                Path('/content/drive/MyDrive/LLM/data/lennon_2243.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/andreessen_2234.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/lennon_2243.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/storch_2233.txt'),\n","                # Add more validation files as needed\n","            ],\n","            'test': [\n","                Path('/content/drive/MyDrive/LLM/data/graves_2244.txt'),\n","                #Path('/content/drive/MyDrive/LLM/data/test_file2.txt'),\n","                # Add more test files as needed\n","            ]\n","        }\n","\n","    def calculate_vocab_size(self):\n","      \"\"\"Calculates vocabulary size from all training files.\"\"\"\n","      try:\n","          # Collect all unique characters from all training files\n","          all_chars = set()\n","          for train_path in self.data_paths['train']:\n","              if not train_path.exists():\n","                  logging.error(f\"Training data file does not exist: {train_path}\")\n","                  raise FileNotFoundError(f\"Training data file does not exist: {train_path}\")\n","\n","              with open(train_path, 'r', encoding='utf-8') as f:\n","                  text = f.read()\n","                  all_chars.update(set(text))\n","\n","          # Sort the unique characters and add special tokens\n","          unique_chars = sorted(all_chars)\n","          special_chars = ['\\n', '\\t', ' ', '_', '[', ']', '(', ')', '{', '}', '*', '/', '\\\\', '|',\n","                          '@', '#', '$', '%', '^', '&', '+', '=', '`', '~', '<pad>', '<extra>']\n","\n","          # Add special characters that aren't already in unique_chars\n","          for char in special_chars:\n","              if char not in unique_chars:\n","                  unique_chars.append(char)\n","\n","          self.setup_tokenizer(unique_chars)\n","\n","          print(f\"Vocabulary created with {self.vocab_size} characters including special characters\")\n","          print(f\"Special characters included: {[c for c in special_chars if c in self.char_to_idx]}\")\n","\n","      except Exception as e:\n","          logging.error(f\"Error during vocabulary calculation: {e}\")\n","          raise\n","\n","      return self.vocab_size\n","\n","    def load_data(self):\n","        \"\"\"Load and preprocess text data from multiple files into properly structured tensors.\"\"\"\n","        try:\n","            # First calculate vocabulary from all training files\n","            self.calculate_vocab_size()\n","\n","            # Process each split (train, val, test)\n","            for split in ['train', 'val', 'test']:\n","                all_indices = []\n","\n","                # Process each file in the split\n","                for file_path in self.data_paths[split]:\n","                    logging.info(f\"Loading {split} data from {file_path}\")\n","                    try:\n","                        with open(file_path, 'r', encoding='utf-8') as f:\n","                            text = f.read()\n","\n","                        # Convert text to indices\n","                        unknown_chars = set()\n","                        file_indices = []\n","                        for c in text:\n","                            if c in self.char_to_idx:\n","                                file_indices.append(self.char_to_idx[c])\n","                            else:\n","                                unknown_chars.add(c)\n","                                file_indices.append(self.char_to_idx[' '])\n","\n","                        if unknown_chars:\n","                            logging.warning(f\"Characters not in vocabulary in {file_path}: {unknown_chars}\")\n","\n","                        all_indices.extend(file_indices)\n","\n","                    except Exception as e:\n","                        logging.error(f\"Error processing file {file_path}: {str(e)}\")\n","                        raise\n","\n","                # Convert combined indices to tensor\n","                data = torch.tensor(all_indices, dtype=torch.long, device=self.config.device)\n","\n","                # Create input-target pairs\n","                n = len(data) - self.config.block_size\n","                if n <= 0:\n","                    raise ValueError(f\"Combined text in {split} split is shorter than block_size\")\n","\n","                input_ids = torch.stack([data[i:i+self.config.block_size] for i in range(0, n, self.config.block_size)])\n","                target_ids = torch.stack([data[i+1:i+self.config.block_size+1] for i in range(0, n, self.config.block_size)])\n","\n","                # Store as tuple of (inputs, targets)\n","                if split == 'train':\n","                    self.train_data = (input_ids, target_ids)\n","                elif split == 'val':\n","                    self.val_data = (input_ids, target_ids)\n","                else:\n","                    self.test_data = (input_ids, target_ids)\n","\n","                logging.info(f\"Loaded {split} data with {len(input_ids)} sequences from {len(self.data_paths[split])} files\")\n","\n","        except Exception as e:\n","            logging.error(f\"Error loading data: {str(e)}\")\n","            raise\n","\n","    def setup_tokenizer(self, unique_chars: List[str]):\n","        \"\"\"Set up the tokenizer mapping.\"\"\"\n","        # Create char_to_idx and idx_to_char mappings\n","        self.char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n","        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n","        self.vocab_size = len(self.char_to_idx)\n","\n","        logging.info(f\"Vocabulary size: {self.vocab_size}\")\n","\n","    def encode(self, text: str) -> torch.Tensor:\n","        \"\"\"Encode text into tensor of indices.\"\"\"\n","        try:\n","            # Ensure all characters are in the vocabulary\n","            indices = [self.char_to_idx[char] for char in text]\n","            return torch.tensor(indices, dtype=torch.long, device=self.config.device)\n","        except KeyError as e:\n","            raise ValueError(f\"Character '{e.args[0]}' not found in vocabulary. Update the vocabulary to include all characters.\")\n","\n","    def get_batch(self, split: str, batch_size: int):\n","        \"\"\"Retrieve a batch of data for training or evaluation.\"\"\"\n","        # Get the appropriate dataset\n","        if split == 'train':\n","            data = self.train_data\n","        elif split == 'val':\n","            data = self.val_data\n","        elif split == 'test':\n","            data = self.test_data\n","        else:\n","            raise ValueError(\"Split must be 'train', 'val', or 'test'\")\n","\n","        # Validate data\n","        if data is None:\n","            raise ValueError(f\"{split.capitalize()} data is not loaded.\")\n","\n","        try:\n","            # Assuming data is a tuple of (inputs, targets)\n","            if not isinstance(data, tuple) or len(data) != 2:\n","                raise ValueError(f\"Data format error: expected tuple of (inputs, targets), got {type(data)}\")\n","\n","            inputs, targets = data\n","\n","            if not isinstance(inputs, torch.Tensor):\n","                raise ValueError(f\"Inputs must be a tensor, got {type(inputs)}\")\n","\n","            # Get random indices for batch\n","            max_idx = max(0, len(inputs) - self.config.block_size - 1)\n","            if max_idx <= 0:\n","                raise ValueError(f\"Data length ({len(inputs)}) must be greater than block_size ({self.config.block_size})\")\n","\n","            idx = torch.randint(0, max_idx, (batch_size,), device=self.config.device)\n","\n","            # Select batch data and ensure proper dimensions\n","            x = inputs[idx]\n","            y = targets[idx]\n","\n","            return x, y\n","\n","        except Exception as e:\n","            logging.error(f\"Error in get_batch: {str(e)}\")\n","            raise"]},{"cell_type":"markdown","source":[],"metadata":{"id":"2-hT9sIORXPj"}},{"cell_type":"code","source":["# RUN FOR TRAINING and INFERENCE\n","\n","from dataclasses import dataclass\n","from typing import Optional, Dict, Any, Union\n","import logging\n","import json\n","from pathlib import Path\n","import torch\n","import time\n","import psutil\n","from datetime import timedelta\n","import threading\n","\n","@dataclass\n","class ModelConfig:\n","    save_dir: Union[str, Path] = Path(\"/content/drive/MyDrive/LLM/checkpoints\")\n","    vocab_size: int = 0\n","    block_size: int = 64              # Context length\n","    n_embed: int = 64                 # Embedding dimension\n","    n_head: int = 4                   # Number of attention heads\n","    n_layer: int = 4                  # Number of transformer layers\n","    ff_dim: int = 256                 # Feed-forward dimension\n","    dropout: float = 0.1              # Dropout rate\n","    bias: bool = True\n","    flash_attn: bool = True\n","    head_dim: int = 64\n","    attn_pdrop: float = 0.1\n","    resid_pdrop: float = 0.1\n","    embd_pdrop: float = 0.1\n","\n","    # Update existing parameters\n","    ff_dim: int = None  # Will be set to 4 * n_embed\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        if self.ff_dim is None:\n","            self.ff_dim = 4 * self.n_embed\n","\n","\n","    # Training Parameters\n","    batch_size: int = 128\n","    gradient_accumulation_steps: int = 1\n","    learning_rate: float = 1e-3\n","    min_learning_rate: float = 1e-3\n","    weight_decay: float = 0.1\n","    epochs: int = 10\n","    warmup_steps: int = 200            # Number of warmup steps\n","    max_grad_norm: float = 1.0\n","    within_epoch_loss_threshold: float = 0.7  # New parameter for early stopping within epoch\n","    eval_steps: int = 100  # Check loss every N steps\n","\n","    # Learning Rate Schedule\n","    lr_schedule: str = 'cosine_with_warmup'\n","    lr_decay_epochs: int = 8\n","\n","    # Memory Optimization\n","    use_amp: bool = True\n","    amp_dtype: torch.dtype = torch.bfloat16  # Changed from float16\n","    dtype: torch.dtype = torch.bfloat16      # Changed from float32\n","    use_gradient_checkpointing: bool = False\n","\n","    # Data Loading\n","    pin_memory: bool = True\n","    num_workers: int = 8\n","    prefetch_factor: int = 5\n","\n","    # Generation Parameters\n","    gen_temperature: float = 0.8\n","    max_gen_tokens: int = 256\n","    top_k: int = 50\n","    top_p: float = 0.9\n","\n","    # Early Stopping\n","    patience: int = 5\n","    min_delta: float = 1e-4\n","    loss_threshold: float = 1.0\n","\n","    # Evaluation and Checkpointing\n","    eval_every: int = 1000\n","    save_every: int = 1\n","    keep_last_n_checkpoints: int = 5\n","\n","    # System Parameters\n","    device: str = \"cuda\"\n","\n","    def __post_init__(self):\n","        \"\"\"Initialize derived parameters and create directories\"\"\"\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.dtype = torch.bfloat16 if self.device == \"cuda\" else torch.float32  # Changed from float16\n","\n","        # Create save directory\n","        self.save_dir = Path(\"/content/drive/MyDrive/LLM/checkpoints\")\n","        self.save_dir.mkdir(parents=True, exist_ok=True)\n","\n","    def print_params(self) -> None:\n","        \"\"\"Print model and training parameters\"\"\"\n","        logging.info(\"Model Parameters:\")\n","        for key, value in self.__dict__.items():\n","            logging.info(f\"  {key}: {value}\")\n","\n","    def setup_logging(self, iteration: int) -> None:\n","        \"\"\"Configure logging system\"\"\"\n","        log_dir = Path('logs')\n","        log_dir.mkdir(parents=True, exist_ok=True)\n","\n","        log_file = log_dir / f'training_{iteration}.log'\n","\n","        logging.basicConfig(\n","            level=logging.INFO,\n","            format='%(asctime)s - %(levelname)s - %(message)s',\n","            handlers=[\n","                logging.StreamHandler(),\n","                logging.FileHandler(log_file)\n","            ]\n","        )\n","\n","        # Log parameters\n","        logging.info(f\"PyTorch version: {torch.__version__}\")\n","        logging.info(f\"CUDA available: {torch.cuda.is_available()}\")\n","        if torch.cuda.is_available():\n","            logging.info(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n","            logging.info(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","\n","        # Print model and training parameters\n","        self.print_params()\n","\n","    def save(self, path: Union[str, Path]) -> None:\n","        \"\"\"Save configuration to JSON\"\"\"\n","        path = Path(path)\n","        config_dict = {\n","            k: str(v) if isinstance(v, (Path, torch.dtype)) else v\n","            for k, v in self.__dict__.items()\n","        }\n","        path.parent.mkdir(parents=True, exist_ok=True)\n","        with path.open('w') as f:\n","            json.dump(config_dict, f, indent=4)\n","\n","    @classmethod\n","    def load(cls, path: Union[str, Path]) -> 'ModelConfig':\n","        \"\"\"Load configuration from JSON\"\"\"\n","        path = Path(path)\n","        with path.open('r') as f:\n","            config_dict = json.load(f)\n","\n","        if 'dtype' in config_dict:\n","            config_dict['dtype'] = getattr(torch, config_dict['dtype'].split('.')[-1])\n","        if 'save_dir' in config_dict:\n","            config_dict['save_dir'] = str(config_dict['save_dir'])\n","\n","        return cls(**config_dict)\n","\n","'''\n","class SystemMonitor:\n","    \"\"\"System resource monitoring with memory optimization\"\"\"\n","\n","    def __init__(self):\n","        self.gpu_available = torch.cuda.is_available()\n","        self.start_time = time.time()\n","        self._lock = threading.Lock()\n","        self.history = {\n","            'train_loss': [],\n","            'val_loss': [],\n","            'learning_rate': [],\n","            'epoch_times': []\n","        }\n","        self.epoch_start_time = None\n","\n","    def start_epoch(self):\n","        \"\"\"Mark the start of an epoch\"\"\"\n","        self.epoch_start_time = time.time()\n","\n","    def end_epoch(self):\n","        \"\"\"Mark the end of an epoch and record duration\"\"\"\n","        if self.epoch_start_time is not None:\n","            duration = time.time() - self.epoch_start_time\n","            self.history['epoch_times'].append(duration)\n","            self.epoch_start_time = None\n","\n","    def cleanup(self):\n","        \"\"\"Cleanup any resources\"\"\"\n","        if self.gpu_available:\n","            torch.cuda.empty_cache()\n","'''"],"metadata":{"id":"s-hhOXFb4txj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Section 4: Model Architecture\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","\n","class RotaryEmbedding(nn.Module):\n","    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n","        super().__init__()\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer('inv_freq', inv_freq)\n","        self.max_seq_len_cached = max_position_embeddings\n","\n","        # Initialize cache\n","        t = torch.arange(max_position_embeddings, device=inv_freq.device).type_as(inv_freq)\n","        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.cos_cached = emb.cos()[None, None, :, :]\n","        self.sin_cached = emb.sin()[None, None, :, :]\n","\n","    def forward(self, x, seq_len=None):\n","        if seq_len > self.max_seq_len_cached:\n","            self.max_seq_len_cached = seq_len\n","            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n","            freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","            emb = torch.cat((freqs, freqs), dim=-1)\n","            self.cos_cached = emb.cos()[None, None, :, :]\n","            self.sin_cached = emb.sin()[None, None, :, :]\n","\n","        return (\n","            self.cos_cached[:, :, :seq_len, ...].to(x.device),\n","            self.sin_cached[:, :, :seq_len, ...].to(x.device)\n","        )\n","\n","def rotate_half(x):\n","    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","def apply_rotary_pos_emb(q, k, cos, sin):\n","    q_embed = (q * cos) + (rotate_half(q) * sin)\n","    k_embed = (k * cos) + (rotate_half(k) * sin)\n","    return q_embed, k_embed\n","\n","class CausalSelfAttention(nn.Module):\n","    \"\"\"Multi-head causal self-attention layer with RoPE, gradient checkpointing and flash attention.\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0\n","\n","        # Store necessary dimensions\n","        self.n_embed = config.n_embed\n","        self.n_head = config.n_head\n","        self.head_dim = config.n_embed // config.n_head\n","        self.dropout = config.dropout\n","\n","        # Key, query, value projections for all heads\n","        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias=config.bias)\n","        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n","\n","        # Regularization\n","        self.attn_dropout = nn.Dropout(config.dropout)\n","        self.resid_dropout = nn.Dropout(config.dropout)\n","\n","        # Initialize RoPE\n","        self.rope = RotaryEmbedding(\n","            self.head_dim,\n","            max_position_embeddings=config.block_size\n","        )\n","\n","        # Flash attention support\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            self.register_buffer(\n","                \"mask\",\n","                torch.tril(torch.ones(config.block_size, config.block_size))\n","                .view(1, 1, config.block_size, config.block_size)\n","            )\n","\n","        # Scaling factor for attention\n","        self.scale = 1.0 / math.sqrt(self.head_dim)\n","\n","    def forward(self, x):\n","        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality\n","\n","        # Calculate query, key, values for all heads\n","        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n","\n","        # Reshape for multi-head attention\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)\n","\n","        # Apply RoPE to queries and keys\n","        cos, sin = self.rope(q, seq_len=T)\n","        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n","\n","        # Causal self-attention with flash attention optimization if available\n","        if self.flash:\n","            # Use flash attention for efficiency\n","            y = torch.nn.functional.scaled_dot_product_attention(\n","                q, k, v,\n","                attn_mask=None,\n","                dropout_p=self.dropout if self.training else 0,\n","                is_causal=True,\n","                scale=self.scale\n","            )\n","        else:\n","            # Manual implementation of attention\n","            att = (q @ k.transpose(-2, -1)) * self.scale\n","            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            att = self.attn_dropout(att)\n","            y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n","\n","        # Re-assemble all head outputs side by side\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","\n","        # Output projection\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","class LayerNorm(nn.Module):\n","    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n","    def __init__(self, ndim, bias=True):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class GPTBlock(nn.Module):\n","    \"\"\"Transformer block with optimizations, layer normalization, and feed-forward network.\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        # Layer normalization with optional bias\n","        self.ln1 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln2 = LayerNorm(config.n_embed, bias=config.bias)\n","\n","        # MLP with 4x expansion ratio\n","        self.mlp = nn.Sequential(\n","            nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias),\n","            nn.GELU(),\n","            nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias),\n","            nn.Dropout(config.dropout),\n","        )\n","\n","        # Optional gradient checkpointing\n","        self.use_checkpointing = config.use_gradient_checkpointing\n","\n","        # LayerScale for better training dynamics (optional)\n","        self.layer_scale_1 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","        self.layer_scale_2 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","\n","    def forward(self, x):\n","        # Attention block with residual connection and layer scale\n","        if self.use_checkpointing and x.requires_grad:\n","            attn_output = torch.utils.checkpoint.checkpoint(self.attn, self.ln1(x))\n","        else:\n","            attn_output = self.attn(self.ln1(x))\n","        x = x + self.layer_scale_1.unsqueeze(0).unsqueeze(0) * attn_output\n","\n","        # MLP block with residual connection and layer scale\n","        if self.use_checkpointing and x.requires_grad:\n","            mlp_output = torch.utils.checkpoint.checkpoint(self.mlp, self.ln2(x))\n","        else:\n","            mlp_output = self.mlp(self.ln2(x))\n","        x = x + self.layer_scale_2.unsqueeze(0).unsqueeze(0) * mlp_output\n","\n","        return x\n","\n","class GPT(nn.Module):\n","    \"\"\"GPT Language Model with RoPE, memory optimizations, and advanced techniques.\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        # Initialize transformer components\n","        self.transformer = nn.ModuleDict({\n","            'wte': nn.Embedding(config.vocab_size, config.n_embed),  # Token embeddings\n","            'drop': nn.Dropout(config.dropout),  # Dropout layer\n","            'h': nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)]),  # Transformer blocks\n","            'ln_f': nn.LayerNorm(config.n_embed)  # Final layer normalization\n","        })\n","\n","        # Output layer to predict vocab size\n","        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n","\n","        # Initialize weights\n","        self.apply(self._init_weights)\n","\n","        # Apply special scaled initialization to the residual projections\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n","\n","        # Report number of parameters\n","        print(f\"Number of parameters: {sum(p.numel() for p in self.parameters()) / 1e6:.2f}M\")\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","        elif isinstance(module, LayerNorm):\n","            torch.nn.init.ones_(module.weight)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, RotaryEmbedding):\n","            # Initialize RoPE parameters if any\n","            pass\n","\n","    def forward(self, idx, targets=None):\n","        \"\"\"Forward pass through the model.\"\"\"\n","        device = idx.device\n","        b, t = idx.size()\n","\n","        # Memory efficient forward pass with AMP\n","        with torch.cuda.amp.autocast(enabled=self.config.use_amp, dtype=torch.bfloat16):\n","            # Only token embeddings now, no positional embeddings needed (handled by RoPE)\n","            x = self.transformer.wte(idx)\n","            x = self.transformer.drop(x)\n","\n","            # Pass through transformer blocks\n","            for block in self.transformer.h:\n","                x = block(x)\n","            x = self.transformer.ln_f(x)\n","\n","            # Compute logits and loss if targets are provided\n","            if targets is not None:\n","                logits = self.lm_head(x)\n","                loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n","            else:\n","                logits = self.lm_head(x[:, -1, :])\n","                loss = None\n","\n","        return logits, loss"],"metadata":{"id":"7IlwjJai79ni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class ModelInterface:\n","    \"\"\"Interface for model operations with comprehensive error handling and state management\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"Initialize the interface with configuration and necessary components\"\"\"\n","        self.config = config\n","        self.original_batch_size = config.batch_size  # Store original batch size\n","        self.model = None\n","        self.data_manager = None\n","        self.scaler = torch.cuda.amp.GradScaler(enabled=getattr(config, 'use_amp', True))\n","        self._is_initialized = False\n","        self.checkpoint_info = None\n","        self._initialize()\n","\n","    def _initialize(self):\n","        \"\"\"Safe initialization with resource management and state verification\"\"\"\n","        try:\n","            # Verify and adjust batch size if needed\n","            if self.config.batch_size != self.original_batch_size:\n","                logging.warning(f\"Batch size was modified from {self.original_batch_size} to {self.config.batch_size}\")\n","                self.config.batch_size = self.original_batch_size  # Restore original batch size\n","\n","            # Initialize data manager first\n","            self.data_manager = DataManager(self.config)\n","\n","            # Initialize model with proper device placement\n","            self.model = GPT(self.config)\n","            self.model = self.model.to(self.config.device)\n","\n","            self._is_initialized = True\n","\n","            # Log initialization details\n","            print(f\"Model initialized with {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M parameters\")\n","            print(f\"Using device: {self.config.device}\")\n","            print(f\"Batch size: {self.config.batch_size}\")\n","\n","            # Adjust gradient accumulation based on GPU memory if needed\n","            if self.config.device == \"cuda\":\n","                gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n","                if gpu_mem < 8:  # Only adjust if very limited memory\n","                    self.config.gradient_accumulation_steps = 2\n","                    logging.info(f\"Adjusted gradient accumulation steps to {self.config.gradient_accumulation_steps} due to limited GPU memory\")\n","\n","        except Exception as e:\n","            self.cleanup()\n","            raise RuntimeError(f\"Failed to initialize model interface: {str(e)}\")\n","\n","    def verify_state(self):\n","        \"\"\"Verify that the model and vocabulary are in a valid state\"\"\"\n","        if not self._is_initialized:\n","            raise RuntimeError(\"Model interface not properly initialized\")\n","\n","        if not self.model:\n","            raise RuntimeError(\"Model not loaded\")\n","\n","        if not self.data_manager:\n","            raise RuntimeError(\"Data manager not initialized\")\n","\n","        if not self.data_manager.char_to_idx:\n","            raise RuntimeError(\"Vocabulary mappings not loaded\")\n","\n","        # Verify vocabulary consistency\n","        if self.data_manager.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Vocabulary size mismatch: {self.data_manager.vocab_size} != {self.config.vocab_size}\")\n","\n","        # Verify model vocabulary size matches config\n","        if self.model.config.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Model vocabulary size mismatch: {self.model.config.vocab_size} != {self.config.vocab_size}\")\n","\n","        # Verify batch size hasn't been modified\n","        if self.config.batch_size != self.original_batch_size:\n","            logging.warning(f\"Batch size mismatch detected. Restoring original batch size: {self.original_batch_size}\")\n","            self.config.batch_size = self.original_batch_size\n","\n","    def load_model(self, model_path: Union[str, Path]):\n","        \"\"\"Load a trained model from a checkpoint with comprehensive state restoration\"\"\"\n","        try:\n","            checkpoint = torch.load(model_path, map_location=self.config.device)\n","\n","            # Preserve original batch size before loading checkpoint config\n","            original_batch_size = self.config.batch_size\n","\n","            # First, set up vocabulary mappings before loading model state\n","            if 'config' in checkpoint:\n","                # Get vocab size from checkpoint config\n","                vocab_size = checkpoint['config'].get('vocab_size')\n","                if vocab_size:\n","                    self.config.vocab_size = vocab_size\n","\n","                # Restore original batch size\n","                checkpoint['config']['batch_size'] = original_batch_size\n","\n","                # Set up vocabulary if present in checkpoint\n","                if 'char_to_idx' in checkpoint['config'] and 'idx_to_char' in checkpoint['config']:\n","                    self.data_manager.char_to_idx = checkpoint['config']['char_to_idx']\n","                    self.data_manager.idx_to_char = checkpoint['config']['idx_to_char']\n","                    self.data_manager.vocab_size = len(self.data_manager.char_to_idx)\n","                else:\n","                    # If no vocabulary in checkpoint, calculate it from training data\n","                    self.data_manager.calculate_vocab_size()\n","            else:\n","                raise ValueError(\"Checkpoint missing required config\")\n","\n","            # Now load model state\n","            if 'model_state_dict' in checkpoint:\n","                self.model.load_state_dict(checkpoint['model_state_dict'])\n","            else:\n","                self.model.load_state_dict(checkpoint)\n","\n","            self.model.eval()\n","            self.verify_state()\n","\n","        except Exception as e:\n","            logging.error(f\"Error loading model: {str(e)}\")\n","            raise\n","\n","    def cleanup(self):\n","        \"\"\"Comprehensive cleanup of resources and memory\"\"\"\n","        try:\n","            if hasattr(self, 'model') and self.model is not None:\n","                self.model.cpu()\n","                del self.model\n","                self.model = None\n","\n","            if hasattr(self, 'data_manager') and self.data_manager is not None:\n","                del self.data_manager\n","                self.data_manager = None\n","\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            gc.collect()\n","\n","            self._is_initialized = False\n","            logging.info(\"Model interface cleaned up successfully\")\n","\n","        except Exception as e:\n","            logging.error(f\"Error during cleanup: {str(e)}\")"],"metadata":{"id":"eKMrUoOX8EKd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"QrL4EjmUU9Vl"}},{"cell_type":"code","source":["# RUN FOR TRAINING and INFERENCE\n","\n","import datetime\n","import logging\n","import torch\n","from tqdm import tqdm\n","from pathlib import Path\n","from contextlib import nullcontext\n","import gc\n","\n","class Trainer:\n","    \"\"\"Handles model training and optimization\"\"\"\n","\n","    def __init__(self, model: GPT, data_manager: DataManager, config: ModelConfig):\n","        self.model = model\n","        self.data_manager = data_manager\n","        self.config = config\n","        self.device = config.device\n","        #self.monitor = SystemMonitor()\n","\n","        # Training state\n","        self.epoch = 0\n","        self.global_step = 0\n","        self.best_val_loss = float('inf')\n","        self.no_improvement_count = 0\n","        self.setup_training()\n","\n","    def setup_training(self):\n","        \"\"\"Setup training parameters.\"\"\"\n","        # Ensure data is loaded\n","        if self.data_manager.train_data is None:\n","            logging.error(\"Training data is not loaded. Please load the data before training.\")\n","            raise ValueError(\"Training data is not loaded.\")\n","\n","        self.optimizer = torch.optim.AdamW(\n","            self.model.parameters(),\n","            lr=self.config.learning_rate,\n","            weight_decay=self.config.weight_decay,\n","            betas=(0.9, 0.95)\n","        )\n","\n","        if self.config.use_amp:\n","            self.scaler = torch.cuda.amp.GradScaler()\n","\n","        total_steps = (\n","            self.config.epochs *\n","            len(self.data_manager.train_data) //\n","            (self.config.batch_size * self.config.gradient_accumulation_steps)\n","        )\n","\n","        self.scheduler = self.get_cosine_schedule_with_warmup(\n","            self.optimizer,\n","            self.config.warmup_steps,\n","            total_steps\n","        )\n","\n","        # Now you can safely access len(self.data_manager.train_data)\n","        num_batches = self.config.epochs * len(self.data_manager.train_data) // self.config.batch_size\n","\n","        # Learning rate scheduler\n","        if self.config.lr_schedule == 'cosine_with_warmup':\n","            self.scheduler = self.get_cosine_schedule_with_warmup(\n","                self.optimizer,\n","                self.config.warmup_steps,\n","                self.config.epochs * len(self.data_manager.train_data) // self.config.batch_size\n","            )\n","        else:\n","            self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n","                self.optimizer, mode='min', factor=0.5, patience=5\n","            )\n","\n","        # Gradient scaler for mixed precision training\n","        self.scaler = torch.cuda.amp.GradScaler() if self.config.use_amp else None\n","\n","        # Setup loss tracking\n","        self.train_losses = []\n","        self.val_losses = []\n","\n","    @staticmethod\n","    def get_cosine_schedule_with_warmup(optimizer, warmup_steps, total_steps):\n","        \"\"\"Create a schedule with linear warmup and cosine decay\"\"\"\n","        def lr_lambda(current_step):\n","            if current_step < warmup_steps:\n","                return float(current_step) / float(max(1, warmup_steps))\n","            progress = float(current_step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n","            return max(0.0, 0.5 * (1.0 + math.cos(math.pi * progress)))\n","\n","        return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n","\n","    def save_checkpoint(self, loss: float, is_best: bool = False, custom_path: Path = None):\n","      \"\"\"Save training checkpoint with epoch number and loss rate in the filename\"\"\"\n","      timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","      # Ensure save_dir is a Path object\n","      if not isinstance(self.config.save_dir, Path):\n","          self.config.save_dir = Path(self.config.save_dir)\n","\n","      # Create directories if they don't exist\n","      self.config.save_dir.mkdir(parents=True, exist_ok=True)\n","\n","      # Always save periodic checkpoints and best model\n","      should_save = True  # Remove restrictive conditions\n","\n","      # Format loss for filename\n","      loss_str = f\"{loss:.4f}\".replace('.', '_')\n","\n","      # Use custom path if provided, otherwise use default naming\n","      if custom_path is None:\n","          if is_best:\n","              checkpoint_name = f\"best_model_epoch_{self.epoch + 1}_loss_{loss_str}_{timestamp}.pt\"\n","          else:\n","              checkpoint_name = f\"checkpoint_epoch_{self.epoch + 1}_loss_{loss_str}_{timestamp}.pt\"\n","          checkpoint_path = self.config.save_dir / checkpoint_name\n","      else:\n","          checkpoint_path = custom_path\n","\n","      checkpoint = {\n","          'epoch': self.epoch,\n","          'model_state_dict': self.model.state_dict(),\n","          'optimizer_state_dict': self.optimizer.state_dict(),\n","          'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n","          'loss': loss,\n","          'config': self.config.__dict__,\n","          'global_step': self.global_step,\n","          'best_val_loss': self.best_val_loss,\n","          'train_losses': self.train_losses,\n","          'val_losses': self.val_losses\n","      }\n","\n","      try:\n","          # Save checkpoint\n","          torch.save(checkpoint, str(checkpoint_path))\n","          logging.info(f\"Checkpoint saved: {checkpoint_path}\")\n","\n","          # Save metadata file\n","          metadata_path = checkpoint_path.with_suffix('.txt')\n","          with open(metadata_path, 'w') as f:\n","              f.write(f\"Checkpoint Information:\\n\")\n","              f.write(f\"Epoch: {self.epoch + 1}\\n\")\n","              f.write(f\"Loss: {loss:.6f}\\n\")\n","              f.write(f\"Timestamp: {timestamp}\\n\")\n","              f.write(f\"Best model: {is_best}\\n\")\n","              f.write(f\"Learning rate: {self.optimizer.param_groups[0]['lr']:.6f}\\n\")\n","              # Add other metadata as needed\n","\n","      except Exception as e:\n","          logging.error(f\"Error saving checkpoint: {str(e)}\")\n","          raise\n","\n","    def train(self):\n","        \"\"\"Main training loop\"\"\"\n","        logging.info(\"Starting training\")\n","        self.log_training_params()\n","\n","        # Create checkpoint directory if it doesn't exist\n","        checkpoint_dir = Path('/content/drive/MyDrive/LLM/checkpoints') if is_colab() else Path('./LLM/checkpoints')\n","        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n","\n","        current_val_loss = float('inf')  # Initialize val_loss\n","\n","        try:\n","            for epoch in range(self.epoch, self.config.epochs):\n","                self.epoch = epoch\n","                logging.info(f\"Epoch {self.epoch + 1}/{self.config.epochs} starting...\")\n","\n","                try:\n","                    # Train for one epoch\n","                    train_loss = self.train_epoch()\n","                    self.train_losses.append(train_loss)\n","                    current_val_loss = self.evaluate('val')  # Store val_loss\n","                    self.val_losses.append(current_val_loss)\n","\n","                    # Save periodic checkpoint\n","                    self.save_checkpoint(current_val_loss, False)\n","\n","                    # Check if this is the best model\n","                    if current_val_loss < self.best_val_loss:\n","                        self.best_val_loss = current_val_loss\n","                        self.save_checkpoint(current_val_loss, True)\n","                        logging.info(f\"New best validation loss: {current_val_loss:.4f}\")\n","\n","                    # Early stopping check\n","                    if self.no_improvement_count >= self.config.patience:\n","                        logging.info(f\"Early stopping triggered after {epoch + 1} epochs\")\n","                        break\n","\n","                except Exception as e:\n","                    logging.error(f\"Error in epoch {epoch + 1}: {str(e)}\")\n","                    # Save checkpoint on error\n","                    error_path = checkpoint_dir / f\"error_checkpoint_epoch_{epoch + 1}.pt\"\n","                    self.save_checkpoint(current_val_loss, False, custom_path=error_path)\n","                    raise\n","\n","        except KeyboardInterrupt:\n","            logging.info(\"Training interrupted by user\")\n","            interrupt_path = checkpoint_dir / f\"interrupted_checkpoint_epoch_{self.epoch + 1}.pt\"\n","            self.save_checkpoint(current_val_loss, False, custom_path=interrupt_path)\n","\n","        finally:\n","            # Save final checkpoint\n","            final_path = checkpoint_dir / f\"final_checkpoint_epoch_{self.epoch + 1}.pt\"\n","            self.save_checkpoint(current_val_loss, False, custom_path=final_path)\n","\n","            # Final evaluation\n","            test_loss = self.evaluate('test')\n","            logging.info(f\"Final test loss: {test_loss:.4f}\")\n","\n","    def _cleanup_old_checkpoints(self):\n","        \"\"\"Remove old checkpoints keeping only the latest ones based on epoch intervals\"\"\"\n","        try:\n","            checkpoints = []\n","            for checkpoint in self.config.save_dir.glob(\"checkpoint_epoch_*.pt\"):\n","                # Extract epoch and loss from filename\n","                parts = checkpoint.stem.split('_')\n","                try:\n","                    epoch_num = int(parts[parts.index('epoch') + 1])\n","                    loss_val = float(parts[parts.index('loss') + 1].replace('_', '.'))\n","                    checkpoints.append((checkpoint, epoch_num, loss_val))\n","                except (ValueError, IndexError):\n","                    logging.warning(f\"Couldn't parse epoch/loss from checkpoint name: {checkpoint}\")\n","                    continue\n","\n","            # Sort checkpoints by epoch number\n","            checkpoints.sort(key=lambda x: x[1])  # Sort by epoch number\n","\n","            # Always keep first and last checkpoint\n","            checkpoints_to_keep = set()\n","            if checkpoints:\n","                checkpoints_to_keep.add(checkpoints[0][0])  # First checkpoint\n","                checkpoints_to_keep.add(checkpoints[-1][0])  # Last checkpoint\n","\n","            # Keep checkpoints at intervals specified by keep_last_n_checkpoints\n","            for checkpoint, epoch_num, _ in checkpoints:\n","                if epoch_num % self.config.keep_last_n_checkpoints == 0:\n","                    checkpoints_to_keep.add(checkpoint)\n","\n","            # Keep best model checkpoints (they start with 'best_model')\n","            best_models = list(self.config.save_dir.glob(\"best_model_*.pt\"))\n","            checkpoints_to_keep.update(best_models)\n","\n","            # Remove checkpoints that are not in the keep list\n","            all_checkpoints = set(self.config.save_dir.glob(\"checkpoint_epoch_*.pt\"))\n","            for checkpoint in all_checkpoints - checkpoints_to_keep:\n","                # Remove both the checkpoint and its metadata file\n","                checkpoint.unlink()\n","                metadata_file = checkpoint.with_suffix('.txt')\n","                if metadata_file.exists():\n","                    metadata_file.unlink()\n","                logging.info(f\"Removed checkpoint: {checkpoint}\")\n","\n","        except Exception as e:\n","            logging.warning(f\"Error during checkpoint cleanup: {str(e)}\")\n","\n","    def save_model(self, model_path: Path, eval_loss: float):\n","        \"\"\"Save the trained model with validation loss and epoch in filename\"\"\"\n","        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        loss_str = f\"{eval_loss:.4f}\".replace('.', '_')\n","\n","        # Ensure we're using the correct models directory path\n","        models_dir = Path('/content/drive/MyDrive/LLM/models') if is_colab() else Path('./LLM/models')\n","        models_dir.mkdir(parents=True, exist_ok=True)\n","\n","        # Include epoch number and loss in the filename\n","        unique_model_path = models_dir / f'trained_model_epoch_{self.epoch + 1}_loss_{loss_str}_{timestamp}.pt'\n","\n","        # Save complete model state\n","        save_dict = {\n","            'model_state_dict': self.model.state_dict(),\n","            'config': self.config.__dict__,\n","            'epoch': self.epoch + 1,\n","            'final_loss': eval_loss,\n","            'vocab_size': self.data_manager.vocab_size,\n","            'char_to_idx': self.data_manager.char_to_idx,\n","            'idx_to_char': self.data_manager.idx_to_char,\n","            'timestamp': timestamp\n","        }\n","\n","        torch.save(save_dict, unique_model_path)\n","        logging.info(f\"Model saved to {unique_model_path}\")\n","\n","        # Save metadata\n","        metadata_path = unique_model_path.with_suffix('.txt')\n","        with open(metadata_path, 'w') as f:\n","            f.write(f\"Model Information:\\n\")\n","            f.write(f\"Epoch: {self.epoch + 1}\\n\")\n","            f.write(f\"Final Loss: {eval_loss:.6f}\\n\")\n","            f.write(f\"Training completed at: {timestamp}\\n\")\n","            f.write(f\"Vocabulary size: {self.data_manager.vocab_size}\\n\")\n","            f.write(f\"Model parameters: {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M\\n\")\n","            f.write(\"\\nModel Configuration:\\n\")\n","            for key, value in self.config.__dict__.items():\n","                f.write(f\"{key}: {value}\\n\")\n","\n","        return unique_model_path\n","\n","    def load_checkpoint(self, checkpoint_path: Union[str, Path]):\n","        \"\"\"Load training checkpoint\"\"\"\n","        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n","\n","        self.model.load_state_dict(checkpoint['model_state_dict'])\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n","        self.epoch = checkpoint['epoch']\n","        self.global_step = checkpoint['global_step']\n","\n","        logging.info(f\"Loaded checkpoint from epoch {self.epoch}\")\n","\n","    def train_epoch(self) -> float:\n","      \"\"\"Train for one epoch\"\"\"\n","      self.model.train()\n","      total_loss = 0.0\n","      running_loss = 0.0\n","\n","      try:\n","          # Get number of sequences in training data\n","          n_sequences = len(self.data_manager.train_data[0])  # Access first element of tuple\n","          num_batches = n_sequences // self.config.batch_size\n","\n","          progress_bar = tqdm(range(num_batches),\n","                            desc=f'Epoch {self.epoch + 1}/{self.config.epochs}',\n","                            dynamic_ncols=True)\n","\n","          batch_count = 0\n","          for _ in progress_bar:\n","              if batch_count >= num_batches:\n","                  break\n","\n","              # Get batch\n","              x, y = self.data_manager.get_batch('train', self.config.batch_size)\n","\n","              # Perform training step\n","              batch_loss = self.train_step(x, y)\n","\n","              # Update metrics\n","              total_loss += batch_loss\n","              running_loss += batch_loss\n","              batch_count += 1\n","\n","              # Update progress bar only once per iteration\n","              avg_loss = total_loss / batch_count\n","              progress_bar.set_postfix({\n","                  'loss': f'{batch_loss:.4f}',\n","                  'avg_loss': f'{avg_loss:.4f}',\n","                  'lr': f'{self.optimizer.param_groups[0][\"lr\"]:.6f}'\n","              })\n","\n","              if batch_count % self.config.eval_steps == 0:\n","                  current_avg_loss = running_loss / self.config.eval_steps\n","                  running_loss = 0.0\n","\n","                  if current_avg_loss < self.config.within_epoch_loss_threshold:\n","                      logging.info(f\"Loss threshold reached: {current_avg_loss:.4f}\")\n","                      break\n","\n","          progress_bar.close()\n","          return total_loss / batch_count if batch_count > 0 else float('inf')\n","\n","      except Exception as e:\n","          logging.error(f\"Error in train_epoch: {str(e)}\")\n","          raise\n","\n","    def train_step(self, x: torch.Tensor, y: torch.Tensor) -> float:\n","        \"\"\"Perform one training step with gradient accumulation\"\"\"\n","        accumulated_loss = 0.0\n","        micro_batch_size = self.config.batch_size // self.config.gradient_accumulation_steps\n","\n","        for i in range(self.config.gradient_accumulation_steps):\n","            start_idx = i * micro_batch_size\n","            end_idx = start_idx + micro_batch_size\n","            micro_x = x[start_idx:end_idx]\n","            micro_y = y[start_idx:end_idx]\n","\n","            with torch.cuda.amp.autocast() if self.config.use_amp else nullcontext():\n","                logits, loss = self.model(micro_x, targets=micro_y)\n","                loss = loss / self.config.gradient_accumulation_steps\n","\n","            accumulated_loss += loss.item() * self.config.gradient_accumulation_steps\n","\n","            if self.config.use_amp:\n","                self.scaler.scale(loss).backward()\n","            else:\n","                loss.backward()\n","\n","            if (i + 1) % self.config.gradient_accumulation_steps == 0:\n","                if self.config.max_grad_norm > 0:\n","                    if self.config.use_amp:\n","                        self.scaler.unscale_(self.optimizer)\n","                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n","\n","                if self.config.use_amp:\n","                    self.scaler.step(self.optimizer)\n","                    self.scaler.update()\n","                else:\n","                    self.optimizer.step()\n","\n","                self.optimizer.zero_grad(set_to_none=True)\n","\n","                if isinstance(self.scheduler, torch.optim.lr_scheduler.LambdaLR):\n","                    self.scheduler.step()\n","\n","                self.global_step += 1\n","\n","        return accumulated_loss\n","\n","    @torch.no_grad()\n","    def evaluate(self, split: str = 'val') -> float:\n","        \"\"\"Evaluate model on validation or test set\"\"\"\n","        was_training = self.model.training\n","        self.model.eval()\n","\n","        try:\n","            total_loss = 0.0\n","            num_batches = 0\n","\n","            data = getattr(self.data_manager, f'{split}_data')\n","            total_samples = data[0].size(0)\n","\n","            for i in range(0, total_samples, self.config.batch_size):\n","                batch_x = data[0][i:i+self.config.batch_size]\n","                batch_y = data[1][i:i+self.config.batch_size]\n","\n","                if len(batch_x) == 0:\n","                    continue\n","\n","                _, loss = self.model(batch_x.to(self.device),\n","                                  targets=batch_y.to(self.device))\n","                total_loss += loss.item()\n","                num_batches += 1\n","\n","            return total_loss / num_batches if num_batches > 0 else float('inf')\n","\n","        finally:\n","            # Restore original training state\n","            self.model.train(was_training)\n","\n","print(\"Section 5: Training system setup complete\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xjS4oImYf9Jl","executionInfo":{"status":"ok","timestamp":1736884661758,"user_tz":360,"elapsed":3,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"2c5b3416-aea5-49b6-ca99-ce3eb8822475"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Section 5: Training system setup complete\n"]}]},{"cell_type":"code","source":["# RUN FOR TRAINING -> Pipeline!\n","\n","import logging\n","import torch\n","import datetime\n","from pathlib import Path\n","from typing import Union\n","import gc\n","\n","class TrainingPipeline:\n","    \"\"\"Handles the overall training pipeline.\"\"\"\n","\n","    def __init__(self, model: GPT, config: ModelConfig):\n","        self.model = model\n","        self.config = config\n","        self.data_manager = DataManager(config)  # Instantiate the DataManager\n","\n","        # Load the datasets using DataManager\n","        self.data_manager.load_data()\n","\n","        # Verify data is loaded\n","        if self.data_manager.train_data is None:\n","            raise ValueError(\"Training data failed to load\")\n","\n","        # Initialize the Trainer with loaded data\n","        self.trainer = Trainer(self.model, self.data_manager, self.config)\n","\n","    def run(self):\n","        \"\"\"Run the training pipeline.\"\"\"\n","        try:\n","            # Log training parameters before starting training\n","            self.log_training_params()\n","\n","            # Step 3: Train the model\n","            logging.info(\"Starting training...\")\n","            self.trainer.train()  # This will handle the entire training process\n","\n","        except ValueError as ve:\n","            logging.error(f\"ValueError: {str(ve)}\")\n","            raise\n","        except FileNotFoundError as fnf_error:\n","            logging.error(f\"File not found: {str(fnf_error)}\")\n","            raise\n","        except torch.cuda.OutOfMemoryError:\n","            logging.error(\"CUDA out of memory. Reduce batch size or use a smaller model.\")\n","            raise\n","        except Exception as e:\n","            logging.error(f\"Error running training pipeline: {str(e)}\")\n","            raise\n","\n","    def train_and_evaluate(self):\n","        \"\"\"Main function to train and evaluate the model.\"\"\"\n","        model_path = Path('/content/drive/MyDrive/LLM/models')\n","        checkpoint_path = Path('/content/drive/MyDrive/LLM/checkpoints')\n","\n","        for path in [model_path, checkpoint_path]:\n","            path.mkdir(parents=True, exist_ok=True)\n","\n","        try:\n","            # Print final derived configuration\n","            print(\"\\nFinal Configuration:\")\n","            for key, value in vars(self.config).items():\n","                print(f\"{key}: {value}\")\n","            print(\"-\" * 50)\n","\n","            print(\"\\nStarting training with:\")\n","            print(f\"- Model size: {sum(p.numel() for p in self.model.parameters()) / 1e6:.2f}M parameters\")\n","            print(f\"- Device: {self.config.device}\")\n","            print(f\"- Batch size: {self.config.batch_size}\")\n","            print(f\"- Learning rate: {self.config.learning_rate}\")\n","\n","            # Train model\n","            best_val_loss = float('inf')\n","            for epoch in range(self.config.epochs):\n","                train_loss = self.trainer.train_epoch()\n","                val_loss = self.trainer.evaluate('val')\n","\n","                # Save checkpoint after each epoch\n","                is_best = val_loss < best_val_loss\n","                if is_best:\n","                    best_val_loss = val_loss\n","                self.trainer.save_checkpoint(val_loss, is_best)\n","\n","                print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n","\n","            # Final evaluation and save\n","            final_eval_loss = self.trainer.evaluate('val')\n","            print(f\"\\nFinal validation loss: {final_eval_loss:.4f}\")\n","\n","            # Save final model\n","            final_model_path = self.save_model(model_path, final_eval_loss)\n","            print(f\"\\nFinal model saved to: {final_model_path}\")\n","\n","        except KeyboardInterrupt:\n","            print(\"\\nInterrupted by user. Saving current state...\")\n","            current_val_loss = self.trainer.evaluate('val')\n","            self.trainer.save_checkpoint(current_val_loss, False)\n","            self.save_model(model_path, current_val_loss)\n","\n","        except Exception as e:\n","            print(f\"\\nError occurred: {str(e)}\")\n","            raise\n","\n","        finally:\n","            print(\"\\nCleaning up...\")\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","    def save_model(self, model_path: Path, eval_loss: float):\n","        \"\"\"Save the trained model with validation loss in filename.\"\"\"\n","        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        loss_str = f\"{eval_loss:.4f}\".replace('.', '_')  # Convert 1.3369 to 1_3369\n","\n","        # Ensure we're using the correct models directory path\n","        models_dir = Path('/content/drive/MyDrive/LLM/models') if is_colab() else Path('./LLM/models')\n","        models_dir.mkdir(parents=True, exist_ok=True)\n","\n","        unique_model_path = models_dir / f'trained_model_loss_{loss_str}_{timestamp}.pt'\n","\n","        # Save complete model state\n","        save_dict = {\n","            'model_state_dict': self.model.state_dict(),\n","            'config': self.config.__dict__,\n","            'final_loss': eval_loss,\n","            'vocab_size': self.data_manager.vocab_size,\n","            'char_to_idx': self.data_manager.char_to_idx,\n","            'idx_to_char': self.data_manager.idx_to_char\n","        }\n","\n","        torch.save(save_dict, unique_model_path)\n","        logging.info(f\"Model saved to {unique_model_path}\")\n","\n","        # Save metadata\n","        metadata_path = unique_model_path.with_suffix('.txt')\n","        with open(metadata_path, 'w') as f:\n","            f.write(f\"Training completed at: {timestamp}\\n\")\n","            f.write(f\"Final validation loss: {eval_loss:.4f}\\n\")\n","            f.write(f\"Vocabulary size: {self.data_manager.vocab_size}\\n\")\n","            f.write(f\"Model parameters: {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M\\n\")\n","            f.write(\"\\nModel Configuration:\\n\")\n","            for key, value in vars(self.config).items():\n","                f.write(f\"{key}: {value}\\n\")\n","\n","        return unique_model_path"],"metadata":{"id":"WuHyArzcgTBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    try:\n","        # Initialize the configuration with optimized values for small LLM\n","        config = ModelConfig(\n","            # Model Architecture\n","            vocab_size=0,  # Will be updated after data loading\n","            block_size=256,  # Context window size\n","            n_layer=6,     # Transformer layers\n","            n_head=8,      # Attention heads\n","            n_embed=384,   # Embedding dimension\n","            ff_dim=1536,   # Feed-forward dimension (4x n_embed)\n","            head_dim=64,   # New: dimension per attention head\n","\n","            # Regularization\n","            dropout=0.2,\n","            attn_pdrop=0.1,   # New: specific attention dropout\n","            resid_pdrop=0.1,  # New: residual dropout\n","            embd_pdrop=0.1,   # New: embedding dropout\n","            weight_decay=0.1,\n","\n","            # Architecture Features\n","            bias=True,\n","            flash_attn=True,  # New: use flash attention if available\n","            use_gradient_checkpointing=True,  # Memory efficiency\n","\n","            # Training Parameters\n","            batch_size=16,\n","            gradient_accumulation_steps=2,\n","            learning_rate=0.0001,\n","            min_learning_rate=1e-5,  # New: minimum learning rate\n","            warmup_steps=500,\n","            epochs=100,\n","            lr_decay_epochs=30,\n","\n","            # Early Stopping\n","            loss_threshold=0.3,\n","            within_epoch_loss_threshold=0.3,\n","            patience=5,        # New: early stopping patience\n","            min_delta=1e-4,   # New: minimum change for improvement\n","\n","            # Evaluation and Checkpointing\n","            eval_steps=1000,\n","            save_every=5,\n","            keep_last_n_checkpoints=5,\n","\n","            # Generation Parameters\n","            gen_temperature=0.8,\n","            max_gen_tokens=64,\n","            top_k=50,\n","            top_p=0.9,        # New: nucleus sampling parameter\n","\n","            # System and Memory\n","            device='cuda' if torch.cuda.is_available() else 'cpu',\n","            pin_memory=True,\n","            num_workers=8,\n","            prefetch_factor=2,\n","\n","            # Precision\n","            use_amp=True,\n","            amp_dtype=torch.bfloat16,\n","            dtype=torch.bfloat16\n","        )\n","\n","        # Initialize data pipeline with error handling\n","        logging.info(\"Initializing data manager...\")\n","        data_manager = DataManager(config)\n","\n","        # Load and process data\n","        logging.info(\"Loading and processing data...\")\n","        data_manager.load_data()\n","\n","        # Calculate vocabulary size\n","        vocab_size = data_manager.calculate_vocab_size()\n","        logging.info(f\"Calculated vocabulary size: {vocab_size}\")\n","\n","        # Analyze vocabulary\n","        unique_characters = list(data_manager.char_to_idx.keys())\n","        logging.info(f\"Found {len(unique_characters)} unique characters\")\n","\n","        if logging.getLogger().isEnabledFor(logging.DEBUG):\n","            logging.debug(\"Unique characters:\")\n","            logging.debug(unique_characters)\n","\n","        # Update config with calculated vocab size\n","        config.vocab_size = vocab_size\n","        logging.info(f\"Updated config vocabulary size: {config.vocab_size}\")\n","\n","        # Initialize model with memory optimization\n","        logging.info(\"Initializing model...\")\n","        with torch.cuda.amp.autocast(enabled=config.use_amp):\n","            model = GPT(config).to(config.device)\n","\n","        # Log model parameters\n","        total_params = sum(p.numel() for p in model.parameters())\n","        logging.info(f\"Model initialized with {total_params/1e6:.2f}M parameters\")\n","\n","        # Optional: Print model architecture in debug mode\n","        if logging.getLogger().isEnabledFor(logging.DEBUG):\n","            logging.debug(\"Model Architecture:\")\n","            logging.debug(str(model))\n","\n","        # Initialize and run training pipeline with error handling\n","        logging.info(\"Initializing training pipeline...\")\n","        pipeline = TrainingPipeline(model, config)\n","\n","        # Run training with resource monitoring\n","        logging.info(\"Starting training...\")\n","        try:\n","            pipeline.train_and_evaluate()\n","        except KeyboardInterrupt:\n","            logging.info(\"Training interrupted by user. Saving checkpoint...\")\n","            pipeline.trainer.save_checkpoint(\n","                pipeline.trainer.evaluate('val'),\n","                is_best=False,\n","                custom_path=Path(config.save_dir) / \"interrupted_checkpoint.pt\"\n","            )\n","\n","        logging.info(\"Training completed successfully\")\n","\n","    except Exception as e:\n","        logging.error(f\"Error during execution: {str(e)}\", exc_info=True)\n","        raise\n","    finally:\n","        # Cleanup\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","        gc.collect()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"E8rpu9PRvfKv","outputId":"83ceb962-c3d6-44ce-b577-c3cabe6a0b8e","executionInfo":{"status":"ok","timestamp":1736884898390,"user_tz":360,"elapsed":236633,"user":{"displayName":"Andrew","userId":"07330014611798335629"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of parameters: 10.73M\n","\n","Final Configuration:\n","save_dir: /content/drive/MyDrive/LLM/checkpoints\n","vocab_size: 100\n","block_size: 256\n","n_embed: 384\n","n_head: 8\n","n_layer: 6\n","ff_dim: 1536\n","dropout: 0.2\n","bias: True\n","flash_attn: True\n","head_dim: 64\n","attn_pdrop: 0.1\n","resid_pdrop: 0.1\n","embd_pdrop: 0.1\n","batch_size: 16\n","gradient_accumulation_steps: 2\n","learning_rate: 0.0001\n","min_learning_rate: 1e-05\n","weight_decay: 0.1\n","epochs: 100\n","warmup_steps: 500\n","max_grad_norm: 1.0\n","within_epoch_loss_threshold: 0.3\n","eval_steps: 1000\n","lr_schedule: cosine_with_warmup\n","lr_decay_epochs: 30\n","use_amp: True\n","amp_dtype: torch.bfloat16\n","dtype: torch.bfloat16\n","use_gradient_checkpointing: True\n","pin_memory: True\n","num_workers: 8\n","prefetch_factor: 2\n","gen_temperature: 0.8\n","max_gen_tokens: 64\n","top_k: 50\n","top_p: 0.9\n","patience: 5\n","min_delta: 0.0001\n","loss_threshold: 0.3\n","eval_every: 1000\n","save_every: 5\n","keep_last_n_checkpoints: 5\n","device: cuda\n","--------------------------------------------------\n","\n","Starting training with:\n","- Model size: 10.73M parameters\n","- Device: cuda\n","- Batch size: 16\n","- Learning rate: 0.0001\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/100: 100%|██████████| 546/546 [00:41<00:00, 13.31it/s, loss=3.9880, avg_loss=5.1647, lr=0.000100]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss = 5.1647, Val Loss = 1.9076\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/100: 100%|██████████| 546/546 [00:40<00:00, 13.46it/s, loss=3.3794, avg_loss=3.7313, lr=0.000100]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Train Loss = 3.7313, Val Loss = 1.6483\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/100: 100%|██████████| 546/546 [00:40<00:00, 13.42it/s, loss=3.1667, avg_loss=3.4077, lr=0.000100]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Train Loss = 3.4077, Val Loss = 1.5358\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/100: 100%|██████████| 546/546 [00:40<00:00, 13.44it/s, loss=3.1511, avg_loss=3.1972, lr=0.000100]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Train Loss = 3.1972, Val Loss = 1.4599\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/100: 100%|██████████| 546/546 [00:40<00:00, 13.36it/s, loss=2.9809, avg_loss=3.0695, lr=0.000100]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Train Loss = 3.0695, Val Loss = 1.4150\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/100:  55%|█████▌    | 301/546 [00:22<00:18, 13.39it/s, loss=2.9770, avg_loss=2.9820, lr=0.000000]\n"]},{"output_type":"stream","name":"stdout","text":["\n","Interrupted by user. Saving current state...\n","\n","Cleaning up...\n"]}]}]}