{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qqqmXrJxt-XO"},"outputs":[],"source":["import os\n","import sys\n","import io\n","import time\n","import random\n","import json\n","import gc\n","import warnings\n","from pathlib import Path\n","from datetime import timedelta\n","import psutil\n","import numpy as np\n","from contextlib import contextmanager, nullcontext\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm  # or tqdm.auto if needed\n","from typing import List, Union, Optional, Dict, Any\n","from dataclasses import dataclass\n","import threading\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import tempfile\n","\n","# Check if running in Google Colab\n","def is_colab():\n","    try:\n","        return 'google.colab' in str(get_ipython())\n","    except NameError:\n","        return False\n","\n","# Mount Google Drive if in Colab\n","if is_colab():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","# Create LLM directory in Drive\n","base_dir = Path('/content/drive/MyDrive/LLM') if is_colab() else Path('./LLM')\n","for dir_name in ['checkpoints', 'models', 'logs', 'configs', 'data']:\n","    (base_dir / dir_name).mkdir(parents=True, exist_ok=True)\n","\n","def is_package_installed(package_name):\n","    try:\n","        __import__(package_name)\n","        return True\n","    except ImportError:\n","        return False\n","\n","if is_colab():\n","    # PyTorch packages\n","    pytorch_packages = ['torch', 'torchvision', 'torchaudio']\n","    pytorch_install = [pkg for pkg in pytorch_packages if not is_package_installed(pkg)]\n","    if pytorch_install:\n","        !pip install {' '.join(pytorch_install)} --index-url https://download.pytorch.org/whl/cu118\n","\n","    # Additional packages\n","    other_packages = ['pynvml', 'nvidia_ml_py3', 'gputil', 'fastapi', 'uvicorn', 'pydantic']\n","    other_install = [pkg for pkg in other_packages if not is_package_installed(pkg)]\n","    if other_install:\n","        !pip install {' '.join(other_install)}\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Section 1: Initial setup and core components complete\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp4ZWu0UjQBh"},"outputs":[],"source":["class DataManager:\n","    def __init__(self, config):\n","        self.config = config\n","        self.char_to_idx = {}\n","        self.idx_to_char = {}\n","        self.train_data: torch.Tensor = None\n","        self.val_data: torch.Tensor = None\n","        self.test_data: torch.Tensor = None\n","        self.vocab_size = 0\n","\n","        # Define the data paths as lists of files\n","        self.data_paths = {\n","            'train': [\n","                Path('/content/drive/MyDrive/LLM/data/huff_2252.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/theo_2253.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/hubbard_2251.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/raekwon_2250.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/pappas_2249.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/trussell_2247.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/fox_2246.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/blagojevich_2245.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/sorin_2242.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/strassman_2241.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/tarantino_2240.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/derek_2239.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/mcphee_2238.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/parks_2236.txt'),\n","            ],\n","            'val': [\n","                Path('/content/drive/MyDrive/LLM/data/lennon_2243.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/andreessen_2234.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/lennon_2243.txt'),\n","                Path('/content/drive/MyDrive/LLM/data/storch_2233.txt'),\n","                # Add more validation files as needed\n","            ],\n","            'test': [\n","                Path('/content/drive/MyDrive/LLM/data/graves_2244.txt'),\n","                #Path('/content/drive/MyDrive/LLM/data/test_file2.txt'),\n","                # Add more test files as needed\n","            ]\n","        }\n","\n","    def update_params():\n","        \"\"\"Prompts the user to update generation parameters.\"\"\"\n","        while True:\n","            try:\n","                temperature = float(input(\"Enter new temperature (0.0-1.0, default 0.7): \") or 0.7)\n","                if not 0.0 <= temperature <= 1.0:\n","                    raise ValueError(\"Temperature must be between 0.0 and 1.0\")\n","\n","                max_tokens = int(input(\"Enter new max tokens (positive integer, default 100): \") or 100)\n","                if max_tokens <= 0:\n","                    raise ValueError(\"Max tokens must be a positive integer\")\n","\n","                top_k = int(input(\"Enter new top k (positive integer, default 50): \") or 50)\n","                if top_k <= 0:\n","                    raise ValueError(\"Top k must be a positive integer\")\n","                break  # Exit the loop if all inputs are valid\n","            except ValueError as e:\n","                print(f\"Invalid input: {e}. Please try again.\")\n","        return temperature, max_tokens, top_k\n","\n","    def calculate_vocab_size(self):\n","        \"\"\"Calculates vocabulary size from all training files.\"\"\"\n","        try:\n","            # Collect all unique characters from all training files\n","            all_chars = set()\n","            for train_path in self.data_paths['train']:\n","                if not train_path.exists():\n","                    print(f\"Training data file does not exist: {train_path}\")\n","                    raise FileNotFoundError(f\"Training data file does not exist: {train_path}\")\n","\n","                with open(train_path, 'r', encoding='utf-8') as f:\n","                    text = f.read()\n","                    all_chars.update(set(text))\n","\n","            # Sort the unique characters and add special tokens\n","            unique_chars = sorted(all_chars)\n","            special_chars = ['\\n', '\\t', ' ', '_', '[', ']', '(', ')', '{', '}', '*', '/', '\\\\', '|',\n","                           '@', '#', '$', '%', '^', '&', '+', '=', '`', '~', '<pad>', '<extra>']\n","\n","            # Add special characters that aren't already in unique_chars\n","            for char in special_chars:\n","                if char not in unique_chars:\n","                    unique_chars.append(char)\n","\n","            self.setup_tokenizer(unique_chars)\n","\n","            print(f\"Vocabulary created with {self.vocab_size} characters including special characters\")\n","            print(f\"Special characters included: {[c for c in special_chars if c in self.char_to_idx]}\")\n","\n","        except Exception as e:\n","            print(f\"Error during vocabulary calculation: {e}\")\n","            raise\n","\n","        return self.vocab_size\n","\n","    def load_and_encode_data(self, data_path: Union[str, Path]) -> torch.Tensor:\n","        \"\"\"Load and encode data from a file.\"\"\"\n","        if not data_path.exists():\n","            print(f\"Data file does not exist: {data_path}\")\n","            raise FileNotFoundError(f\"Data file does not exist: {data_path}\")\n","\n","        with open(data_path, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","\n","        print(f\"Loaded {len(text)} characters from {data_path}.\")\n","        unique_chars = sorted(set(text))\n","\n","        # Add <pad> and <extra> tokens to the vocabulary\n","        unique_chars.extend(['<pad>', '<extra>'])\n","\n","        self.setup_tokenizer(unique_chars)\n","\n","        return self.encode(text)\n","\n","    def setup_tokenizer(self, unique_chars: List[str]):\n","        \"\"\"Set up the tokenizer mapping.\"\"\"\n","        # Create char_to_idx and idx_to_char mappings\n","        self.char_to_idx = {char: idx for idx, char in enumerate(unique_chars)}\n","        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n","        self.vocab_size = len(self.char_to_idx)\n","\n","        print(f\"Vocabulary size: {self.vocab_size}\")\n","\n","    def encode(self, text: str) -> torch.Tensor:\n","        \"\"\"Encode text into tensor of indices.\"\"\"\n","        try:\n","            # Ensure all characters are in the vocabulary\n","            indices = [self.char_to_idx[char] for char in text]\n","            return torch.tensor(indices, dtype=torch.long, device=self.config.device)\n","        except KeyError as e:\n","            raise ValueError(f\"Character '{e.args[0]}' not found in vocabulary. Update the vocabulary to include all characters.\")\n","\n","    def decode(self, indices: torch.Tensor) -> str:\n","        \"\"\"Decode tensor of indices back into text with robust dimension handling.\"\"\"\n","        try:\n","            # Move tensor to CPU and detach from computation graph\n","            indices = indices.detach().cpu()\n","\n","            # Get the shape of the tensor\n","            shape = indices.size()\n","            print(f\"Decode input tensor shape: {shape}\")\n","\n","            # If we have a batch dimension, take the first sequence\n","            if len(shape) > 1:\n","                indices = indices[0]\n","\n","            # Convert to 1D list of indices\n","            index_list = indices.tolist()\n","\n","            # Ensure index_list is flat\n","            if isinstance(index_list, (list, tuple)) and isinstance(index_list[0], (list, tuple)):\n","                index_list = index_list[0]\n","\n","            # Convert indices to characters\n","            result = []\n","            for idx in index_list:\n","                if idx in self.idx_to_char:\n","                    result.append(self.idx_to_char[idx])\n","                else:\n","                    print(f\"Unknown index: {idx}\")\n","                    result.append('<UNK>')\n","\n","            return ''.join(result)\n","\n","        except Exception as e:\n","            print(f\"Decoding error: {str(e)}\")\n","            raise"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-hhOXFb4txj"},"outputs":[],"source":["@dataclass\n","class ModelConfig:\n","    save_dir: Union[str, Path] = Path(\"/content/drive/MyDrive/LLM/checkpoints\")\n","    vocab_size: int = 0\n","    block_size: int = 64              # Context length\n","    n_embed: int = 64                 # Embedding dimension\n","    n_head: int = 4                   # Number of attention heads\n","    n_layer: int = 4                  # Number of transformer layers\n","    ff_dim: int = 256                 # Feed-forward dimension\n","    #dropout: float = 0.1              # Dropout rate\n","    bias: bool = True\n","    flash_attn: bool = True\n","    head_dim: int = 64\n","    #attn_pdrop: float = 0.1\n","    resid_pdrop: float = 0.2\n","    #embd_pdrop: float = 0.1\n","\n","    # Update existing parameters\n","    ff_dim: int = None  # Will be set to 4 * n_embed\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        if self.ff_dim is None:\n","            self.ff_dim = 4 * self.n_embed\n","\n","\n","    # Training Parameters\n","    batch_size: int = 128\n","    gradient_accumulation_steps: int = 1\n","    learning_rate: float = 1e-3\n","    min_learning_rate: float = 1e-3\n","    weight_decay: float = 0.1\n","    epochs: int = 10\n","    warmup_steps: int = 200            # Number of warmup steps\n","    max_grad_norm: float = 1.0\n","    within_epoch_loss_threshold: float = 0.7  # New parameter for early stopping within epoch\n","    eval_steps: int = 100  # Check loss every N steps\n","\n","    # Learning Rate Schedule\n","    lr_schedule: str = 'cosine_with_warmup'\n","    lr_decay_epochs: int = 8\n","\n","    # Memory Optimization\n","    use_amp: bool = True\n","    amp_dtype: torch.dtype = torch.bfloat16  # Changed from float16\n","    dtype: torch.dtype = torch.bfloat16      # Changed from float32\n","    use_gradient_checkpointing: bool = False\n","\n","    # Data Loading\n","    pin_memory: bool = True\n","    num_workers: int = 8\n","    prefetch_factor: int = 5\n","\n","    # Generation Parameters\n","    gen_temperature: float = 0.8\n","    max_gen_tokens: int = 256\n","    top_k: int = 50\n","    top_p: float = 0.9\n","\n","    # Early Stopping\n","    patience: int = 5\n","    min_delta: float = 1e-4\n","    loss_threshold: float = 1.0\n","\n","    # Evaluation and Checkpointing\n","    eval_every: int = 1000\n","    save_every: int = 1\n","    keep_last_n_checkpoints: int = 5\n","\n","    # System Parameters\n","    device: str = \"cuda\"\n","\n","    def __post_init__(self):\n","        \"\"\"Initialize derived parameters and create directories\"\"\"\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.dtype = torch.bfloat16 if self.device == \"cuda\" else torch.float32  # Changed from float16\n","\n","        # Create save directory\n","        self.save_dir = Path(\"/content/drive/MyDrive/LLM/checkpoints\")\n","        self.save_dir.mkdir(parents=True, exist_ok=True)\n","\n","    def print_params(self) -> None:\n","        \"\"\"Print model and training parameters\"\"\"\n","        print(\"Model Parameters:\")\n","        for key, value in self.__dict__.items():\n","            print(f\"  {key}: {value}\")\n","\n","    def save(self, path: Union[str, Path]) -> None:\n","        \"\"\"Save configuration to JSON\"\"\"\n","        path = Path(path)\n","        config_dict = {\n","            k: str(v) if isinstance(v, (Path, torch.dtype)) else v\n","            for k, v in self.__dict__.items()\n","        }\n","        path.parent.mkdir(parents=True, exist_ok=True)\n","        with path.open('w') as f:\n","            json.dump(config_dict, f, indent=4)\n","\n","    @classmethod\n","    def load(cls, path: Union[str, Path]) -> 'ModelConfig':\n","        \"\"\"Load configuration from JSON\"\"\"\n","        path = Path(path)\n","        with path.open('r') as f:\n","            config_dict = json.load(f)\n","\n","        if 'dtype' in config_dict:\n","            config_dict['dtype'] = getattr(torch, config_dict['dtype'].split('.')[-1])\n","        if 'save_dir' in config_dict:\n","            config_dict['save_dir'] = str(config_dict['save_dir'])\n","\n","        return cls(**config_dict)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IlwjJai79ni"},"outputs":[],"source":["class RotaryEmbedding(nn.Module):\n","    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n","        super().__init__()\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer('inv_freq', inv_freq)\n","        self.max_seq_len_cached = max_position_embeddings\n","\n","        # Initialize cache\n","        t = torch.arange(max_position_embeddings, device=inv_freq.device).type_as(inv_freq)\n","        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.cos_cached = emb.cos()[None, None, :, :]\n","        self.sin_cached = emb.sin()[None, None, :, :]\n","\n","    def forward(self, x, seq_len=None):\n","        if seq_len > self.max_seq_len_cached:\n","            self.max_seq_len_cached = seq_len\n","            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n","            freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","            emb = torch.cat((freqs, freqs), dim=-1)\n","            self.cos_cached = emb.cos()[None, None, :, :]\n","            self.sin_cached = emb.sin()[None, None, :, :]\n","\n","        return (\n","            self.cos_cached[:, :, :seq_len, ...].to(x.device),\n","            self.sin_cached[:, :, :seq_len, ...].to(x.device)\n","        )\n","\n","def rotate_half(x):\n","    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","def apply_rotary_pos_emb(q, k, cos, sin):\n","    q_embed = (q * cos) + (rotate_half(q) * sin)\n","    k_embed = (k * cos) + (rotate_half(k) * sin)\n","    return q_embed, k_embed\n","\n","class CausalSelfAttention(nn.Module):\n","    \"\"\"Multi-head causal self-attention layer with RoPE and residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0\n","\n","        # Store necessary dimensions\n","        self.n_embed = config.n_embed\n","        self.n_head = config.n_head\n","        self.head_dim = config.n_embed // config.n_head\n","\n","        # Key, query, value projections for all heads\n","        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias=config.bias)\n","        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n","\n","        # Regularization\n","        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n","\n","        # Initialize RoPE\n","        self.rope = RotaryEmbedding(\n","            self.head_dim,\n","            max_position_embeddings=config.block_size\n","        )\n","\n","        # Flash attention support\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            self.register_buffer(\n","                \"mask\",\n","                torch.tril(torch.ones(config.block_size, config.block_size))\n","                .view(1, 1, config.block_size, config.block_size)\n","            )\n","\n","        # Scaling factor for attention\n","        self.scale = 1.0 / math.sqrt(self.head_dim)\n","\n","    def forward(self, x):\n","        B, T, C = x.size()\n","\n","        # Calculate query, key, values for all heads\n","        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n","\n","        # Reshape for multi-head attention\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","\n","        # Apply RoPE to queries and keys\n","        cos, sin = self.rope(q, seq_len=T)\n","        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n","\n","        # Causal self-attention with flash attention optimization\n","        if self.flash:\n","            y = torch.nn.functional.scaled_dot_product_attention(\n","                q, k, v,\n","                attn_mask=None,\n","                dropout_p=0.0,  # Disabled in favor of residual dropout\n","                is_causal=True,\n","                scale=self.scale\n","            )\n","        else:\n","            att = (q @ k.transpose(-2, -1)) * self.scale\n","            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            y = att @ v\n","\n","        # Re-assemble and project\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","class LayerNorm(nn.Module):\n","    \"\"\"LayerNorm with optional bias\"\"\"\n","    def __init__(self, ndim, bias=True):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class GPTBlock(nn.Module):\n","    \"\"\"Transformer block with residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln1 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln2 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias),\n","            nn.GELU(),\n","            nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias),\n","            nn.Dropout(config.resid_pdrop)\n","        )\n","\n","        # Training optimizations\n","        self.use_checkpointing = config.use_gradient_checkpointing\n","        self.layer_scale_1 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","        self.layer_scale_2 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","\n","    def forward(self, x):\n","        # Attention block\n","        if self.use_checkpointing and x.requires_grad:\n","            attn_output = torch.utils.checkpoint.checkpoint(self.attn, self.ln1(x))\n","        else:\n","            attn_output = self.attn(self.ln1(x))\n","        x = x + self.layer_scale_1.unsqueeze(0).unsqueeze(0) * attn_output\n","\n","        # MLP block\n","        if self.use_checkpointing and x.requires_grad:\n","            mlp_output = torch.utils.checkpoint.checkpoint(self.mlp, self.ln2(x))\n","        else:\n","            mlp_output = self.mlp(self.ln2(x))\n","        x = x + self.layer_scale_2.unsqueeze(0).unsqueeze(0) * mlp_output\n","\n","        return x\n","\n","class GPT(nn.Module):\n","    \"\"\"GPT-like transformer with residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.transformer = nn.ModuleDict({\n","            'wte': nn.Embedding(config.vocab_size, config.n_embed),\n","            'h': nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)]),\n","            'ln_f': nn.LayerNorm(config.n_embed)\n","        })\n","        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n","\n","        # Init\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n","\n","        print(f\"Parameters: {sum(p.numel() for p in self.parameters()) / 1e6:.2f}M\")\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, LayerNorm):\n","            torch.nn.init.ones_(module.weight)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","\n","        with torch.cuda.amp.autocast(enabled=self.config.use_amp, dtype=torch.bfloat16):\n","            x = self.transformer.wte(idx)\n","            for block in self.transformer.h:\n","                x = block(x)\n","            x = self.transformer.ln_f(x)\n","\n","            logits = self.lm_head(x)\n","\n","            # Fixed loss calculation\n","            if targets is not None:\n","                loss = F.cross_entropy(\n","                    logits.view(-1, logits.size(-1)),\n","                    targets.view(-1)\n","                )\n","            else:\n","                # For generation mode\n","                logits = logits[:, -1, :]\n","                loss = None\n","\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx: torch.Tensor, max_new_tokens: int,\n","                temperature: float = 1.0, top_k: Optional[int] = None) -> torch.Tensor:\n","        \"\"\"Generate tokens with shape validation\"\"\"\n","        if idx.dim() != 2:\n","            idx = idx.unsqueeze(0)  # Add batch dimension if not present\n","\n","        for _ in range(max_new_tokens):\n","            # Crop sequence if needed\n","            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n","\n","            # Forward pass\n","            logits, _ = self(idx_cond)\n","\n","            # If logits has 3 dimensions (batch, sequence, vocab), take last token\n","            if logits.dim() == 3:\n","                logits = logits[:, -1, :]\n","\n","            # Apply temperature\n","            logits = logits / temperature\n","\n","            # Apply top-k sampling\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = float('-inf')\n","\n","            # Sample next token\n","            probs = F.softmax(logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","\n","            # Append next token\n","            idx = torch.cat((idx, next_token), dim=1)\n","\n","        return idx"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKMrUoOX8EKd"},"outputs":[],"source":["class ModelInterface:\n","    \"\"\"Interface for model operations with comprehensive error handling and state management\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"Initialize the interface with configuration and necessary components\"\"\"\n","        self.config = config\n","        self.model = None\n","        self.data_manager = None\n","        self._is_initialized = False\n","        self.checkpoint_info = None\n","        self._initialize()\n","\n","    def _initialize(self):\n","        \"\"\"Safe initialization with resource management and state verification\"\"\"\n","        try:\n","            # Initialize data manager first\n","            self.data_manager = DataManager(self.config)\n","\n","            # Initialize model with proper device placement\n","            self.model = GPT(self.config)\n","            self.model = self.model.to(self.config.device)\n","\n","            self._is_initialized = True\n","\n","            # Log initialization details\n","            print(f\"Model initialized with {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M parameters\")\n","            print(f\"Using device: {self.config.device}\")\n","            print(f\"Batch size: {self.config.batch_size}\")\n","\n","            # Adjust gradient accumulation based on GPU memory if needed\n","            if self.config.device == \"cuda\":\n","                gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n","                if gpu_mem < 8:  # Only adjust if very limited memory\n","                    self.config.gradient_accumulation_steps = 2\n","                    print(f\"Adjusted gradient accumulation steps to {self.config.gradient_accumulation_steps} due to limited GPU memory\")\n","\n","        except Exception as e:\n","            self.cleanup()\n","            raise RuntimeError(f\"Failed to initialize model interface: {str(e)}\")\n","\n","    def verify_state(self):\n","        \"\"\"Verify that the model and vocabulary are in a valid state\"\"\"\n","        if not self._is_initialized:\n","            raise RuntimeError(\"Model interface not properly initialized\")\n","\n","        if not self.model:\n","            raise RuntimeError(\"Model not loaded\")\n","\n","        if not self.data_manager:\n","            raise RuntimeError(\"Data manager not initialized\")\n","\n","        if not self.data_manager.char_to_idx:\n","            raise RuntimeError(\"Vocabulary mappings not loaded\")\n","\n","        # Verify vocabulary consistency\n","        if self.data_manager.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Vocabulary size mismatch: {self.data_manager.vocab_size} != {self.config.vocab_size}\")\n","\n","        # Verify model vocabulary size matches config\n","        if self.model.config.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Model vocabulary size mismatch: {self.model.config.vocab_size} != {self.config.vocab_size}\")\n","\n","    def load_model(self, model_path: Union[str, Path]):\n","        \"\"\"Load a trained model from a checkpoint with comprehensive state restoration\"\"\"\n","        try:\n","            checkpoint = torch.load(model_path, map_location=self.config.device)\n","\n","            # First, set up vocabulary mappings before loading model state\n","            if 'config' in checkpoint:\n","                # Get vocab size from checkpoint config\n","                vocab_size = checkpoint['config'].get('vocab_size')\n","                if vocab_size:\n","                    self.config.vocab_size = vocab_size\n","\n","                # Set up vocabulary if present in checkpoint\n","                if 'char_to_idx' in checkpoint['config'] and 'idx_to_char' in checkpoint['config']:\n","                    self.data_manager.char_to_idx = checkpoint['config']['char_to_idx']\n","                    self.data_manager.idx_to_char = checkpoint['config']['idx_to_char']\n","                    self.data_manager.vocab_size = len(self.data_manager.char_to_idx)\n","                else:\n","                    # If no vocabulary in checkpoint, calculate it from training data\n","                    self.data_manager.calculate_vocab_size()\n","            else:\n","                raise ValueError(\"Checkpoint missing required config\")\n","\n","            # Now load model state\n","            if 'model_state_dict' in checkpoint:\n","                self.model.load_state_dict(checkpoint['model_state_dict'])\n","            else:\n","                self.model.load_state_dict(checkpoint)\n","\n","            self.model.eval()\n","            self.verify_state()\n","\n","        except Exception as e:\n","            print(f\"Error loading model: {str(e)}\")\n","            raise\n","\n","    def generate_text(self, prompt: str, max_tokens: int = 100,\n","                     temperature: float = 0.0, top_k: int = 50) -> str:\n","        \"\"\"Generate text with comprehensive error handling and memory management\"\"\"\n","        try:\n","            # Verify state before generation\n","            self.verify_state()\n","\n","            if not prompt:\n","                raise ValueError(\"Empty prompt provided\")\n","\n","            if temperature <= 0:\n","                raise ValueError(\"Temperature must be positive\")\n","\n","            if max_tokens <= 0:\n","                raise ValueError(\"max_tokens must be positive\")\n","\n","            if top_k <= 0:\n","                raise ValueError(\"top_k must be positive\")\n","\n","            # Set model to evaluation mode\n","            self.model.eval()\n","\n","            with torch.no_grad():\n","                try:\n","                    # Encode the prompt\n","                    tokens = self.data_manager.encode(prompt)\n","                except KeyError as e:\n","                    raise ValueError(f\"Character in prompt not in vocabulary: {e}\")\n","\n","                # Ensure proper tensor shape\n","                if tokens.dim() == 1:\n","                    tokens = tokens.unsqueeze(0)\n","\n","                # Move to correct device\n","                tokens = tokens.to(self.config.device)\n","\n","                # Generate text\n","                generated = self.model.generate(\n","                    tokens,\n","                    max_new_tokens=max_tokens,\n","                    temperature=temperature,\n","                    top_k=top_k\n","                )\n","\n","                # Decode the generated tokens\n","                try:\n","                    if generated.dim() == 2:\n","                        generated_text = self.data_manager.decode(generated[0])\n","                    else:\n","                        generated_text = self.data_manager.decode(generated)\n","                except Exception as e:\n","                    raise RuntimeError(f\"Error decoding generated tokens: {str(e)}\")\n","\n","                return generated_text\n","\n","        except Exception as e:\n","            print(f\"Error in text generation: {str(e)}\")\n","            raise\n","        finally:\n","            # Cleanup\n","            if self.config.device == \"cuda\":\n","                torch.cuda.empty_cache()\n","\n","    def cleanup(self):\n","        \"\"\"Comprehensive cleanup of resources and memory\"\"\"\n","        try:\n","            if hasattr(self, 'model') and self.model is not None:\n","                self.model.cpu()\n","                del self.model\n","                self.model = None\n","\n","            if hasattr(self, 'data_manager') and self.data_manager is not None:\n","                del self.data_manager\n","                self.data_manager = None\n","\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            gc.collect()\n","\n","            self._is_initialized = False\n","            print(\"Model interface cleaned up successfully\")\n","\n","        except Exception as e:\n","            print(f\"Error during cleanup: {str(e)}\")\n","\n","    def get_model_info(self) -> Dict[str, Any]:\n","        \"\"\"Return current model and configuration information\"\"\"\n","        return {\n","            'initialized': self._is_initialized,\n","            'device': self.config.device,\n","            'vocab_size': getattr(self.config, 'vocab_size', None),\n","            'model_parameters': sum(p.numel() for p in self.model.parameters())/1e6 if self.model else None,\n","            'has_checkpoint': self.checkpoint_info is not None,\n","            'checkpoint_epoch': self.checkpoint_info.get('epoch') if self.checkpoint_info else None,\n","            'checkpoint_loss': self.checkpoint_info.get('loss') if self.checkpoint_info else None\n","        }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rZsQkWY0ojx3"},"outputs":[],"source":["import gc\n","\n","def run_interactive_chat(model_path: Union[str, Path], temperature=0.7, max_tokens=100, top_k=50):\n","    \"\"\"Load trained model and run interactive chat session\"\"\"\n","    interface = None\n","    try:\n","        # Validate model path\n","        model_path = Path(model_path)\n","        if not model_path.exists():\n","            raise FileNotFoundError(f\"Model checkpoint not found: {model_path}\")\n","\n","        # Determine device\n","        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        print(f\"Using device: {device}\")\n","\n","        # Load checkpoint\n","        try:\n","            checkpoint = torch.load(model_path, map_location=device)\n","        except Exception as e:\n","            raise RuntimeError(f\"Failed to load checkpoint: {e}\")\n","\n","        if 'config' not in checkpoint:\n","            raise ValueError(\"Invalid checkpoint format: missing config\")\n","\n","        # Initialize config with correct device, filtering unwanted config parameters\n","        config_dict = checkpoint['config']\n","        config_dict['device'] = device  # Override device in loaded config\n","\n","        # Remove parameters not in ModelConfig class\n","        unwanted_params = ['save_dir']\n","        for param in unwanted_params:\n","            config_dict.pop(param, None)\n","\n","        config = ModelConfig(**config_dict)\n","\n","        # Initialize interface\n","        interface = ModelInterface(config)\n","        interface.load_model(model_path)\n","\n","        # Use interface's data_manager\n","        data_manager = interface.data_manager\n","\n","        print(f\"\\nModel loaded successfully:\")\n","        model_info = interface.get_model_info()\n","        print(f\"- Model size: {model_info['model_parameters']:.2f}M parameters\")\n","        print(f\"- Device: {config.device}\")\n","        print(f\"- Vocabulary size: {config.vocab_size}\")\n","        print(f\"- Generation parameters:\")\n","        print(f\"  - Temperature: {temperature}\")\n","        print(f\"  - Max tokens: {max_tokens}\")\n","        print(f\"  - Top k: {top_k}\")\n","\n","        # Interactive loop\n","        print(\"\\nStarting interactive session... (type 'quit' to exit, 'params' to update parameters)\")\n","        while True:\n","            try:\n","                prompt = input(\"\\nYou: \").strip()\n","                if not prompt:\n","                    continue\n","\n","                if prompt.lower() == 'quit':\n","                    break\n","\n","                if prompt.lower() == 'params':\n","                    temperature, max_tokens, top_k = data_manager.update_params()\n","                    print(f\"\\nUpdated parameters:\")\n","                    print(f\"- Temperature: {temperature}\")\n","                    print(f\"- Max tokens: {max_tokens}\")\n","                    print(f\"- Top k: {top_k}\")\n","                    continue\n","\n","                response = interface.generate_text(\n","                    prompt,\n","                    max_tokens=max_tokens,\n","                    temperature=temperature,\n","                    top_k=top_k\n","                )\n","                print(\"\\nModel:\", response)\n","\n","            except KeyboardInterrupt:\n","                print(\"\\nInterrupted by user\")\n","                break\n","            except Exception as e:\n","                print(f\"\\nGeneration error: {str(e)}\")\n","                continue\n","\n","    except Exception as e:\n","        print(f\"\\nError initializing chat: {str(e)}\")\n","        raise\n","\n","    finally:\n","        if interface is not None:\n","            interface.cleanup()\n","        print(\"\\nSession ended. Resources cleaned up.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4wcfsQFa_zBA"},"outputs":[],"source":["if __name__ == \"__main__\":\n","     # Specify the model path directly\n","    model_path = '/content/drive/MyDrive/LLM/models/trained_model_loss_1_1983_20250124_191919.pt'\n","\n","    print(f\"Using model checkpoint: {model_path}\")\n","\n","    try:\n","        # Run interactive chat\n","        run_interactive_chat(model_path)\n","    except KeyboardInterrupt:\n","        print(\"\\nInterrupted by user\")\n","    except Exception as e:\n","        print(f\"\\nError: {str(e)}\")\n","    finally:\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"yEDDgwa0gJtQ"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1zHaOvxWM8twT04dcIaplOJUgz_hCQ1BH","timestamp":1736024230358},{"file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","timestamp":1735856996491}],"mount_file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","authorship_tag":"ABX9TyNYqDCZe/LadeyEp51tZZE5"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}