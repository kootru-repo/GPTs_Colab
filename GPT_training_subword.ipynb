{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qqqmXrJxt-XO"},"outputs":[],"source":["import os\n","import sys\n","import io\n","import time\n","import random\n","import json\n","import gc\n","import warnings\n","from pathlib import Path\n","from datetime import timedelta\n","import psutil\n","import numpy as np\n","from contextlib import contextmanager, nullcontext\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm  # or tqdm.auto if needed\n","from typing import List, Union, Optional, Dict, Any\n","from dataclasses import dataclass\n","import threading\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import tempfile\n","\n","\n","# Check if running in Google Colab\n","def is_colab():\n","    try:\n","        return 'google.colab' in str(get_ipython())\n","    except NameError:\n","        return False\n","\n","# Mount Google Drive if in Colab\n","if is_colab():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","# Create LLM directory in Drive\n","base_dir = Path('/content/drive/MyDrive/LLM') if is_colab() else Path('./LLM')\n","for dir_name in ['checkpoints', 'models', 'logs', 'configs', 'data']:\n","    (base_dir / dir_name).mkdir(parents=True, exist_ok=True)\n","\n","def is_package_installed(package_name):\n","    try:\n","        __import__(package_name)\n","        return True\n","    except ImportError:\n","        return False\n","\n","if is_colab():\n","    # PyTorch packages\n","    pytorch_packages = ['torch', 'torchvision', 'torchaudio']\n","    pytorch_install = [pkg for pkg in pytorch_packages if not is_package_installed(pkg)]\n","    if pytorch_install:\n","        !pip install {' '.join(pytorch_install)}\n","\n","    # Additional packages\n","    other_packages = ['pynvml', 'nvidia_ml_py3', 'gputil', 'fastapi', 'uvicorn', 'pydantic', 'sentencepiece']\n","    other_install = [pkg for pkg in other_packages if not is_package_installed(pkg)]\n","    if other_install:\n","        !pip install {' '.join(other_install)}\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Section 1: Initial setup and core components complete\")"]},{"cell_type":"markdown","metadata":{"id":"2-hT9sIORXPj"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s-hhOXFb4txj"},"outputs":[],"source":["@dataclass\n","class ModelConfig:\n","    \"\"\"Configuration for model training and architecture\"\"\"\n","    # Model Architecture\n","    vocab_size: int = 0\n","    block_size: int = 64\n","    n_embed: int = 64\n","    n_head: int = 4\n","    n_layer: int = 4\n","    ff_dim: int = 256\n","    head_dim: int = 32\n","\n","    # Regularization\n","    resid_pdrop: float = 0.2\n","    weight_decay: float = 0.1\n","\n","    # Architecture Features\n","    bias: bool = True\n","    flash_attn: bool = True\n","    use_gradient_checkpointing: bool = False\n","\n","    # Training Parameters\n","    batch_size: int = 32\n","    gradient_accumulation_steps: int = 1\n","    epochs: int = 10\n","    max_grad_norm: float = 1.0\n","    gradient_clip_val: float = 1.0\n","    log_interval: int = 100  # Added this parameter\n","\n","    # Enhanced Learning Rate Parameters\n","    learning_rate: float = 1e-4\n","    min_learning_rate: float = 1e-5\n","    warmup_steps: int = 200\n","    lr_schedule: str = 'cosine_with_warmup'  # Options: 'cosine_with_warmup', 'linear_with_warmup', 'step'\n","    lr_decay_epochs: int = 8  # For step scheduler\n","    warmup_ratio: float = 0.001  # Alternative to warmup_steps (ratio of total training steps)\n","\n","    # Early Stopping\n","    patience: int = 5\n","    min_delta: float = 1e-4\n","    loss_threshold: float = 1.0\n","    within_epoch_loss_threshold: float = 0.3\n","\n","    # Evaluation and Checkpointing\n","    eval_steps: int = 10000\n","    eval_every: int = 10000\n","    save_every: int = 1\n","    keep_last_n_checkpoints: int = 5\n","\n","    # Precision\n","    use_amp: bool = True\n","    amp_dtype: torch.dtype = torch.bfloat16\n","    dtype: torch.dtype = torch.bfloat16\n","\n","    # System Parameters\n","    pin_memory: bool = True\n","    device: str = \"cuda\"\n","    num_workers: int = 8\n","    prefetch_factor: int = 5\n","\n","    # Paths\n","    save_dir: Union[str, Path] = Path(\"/content/drive/MyDrive/LLM/checkpoints\")\n","    tokenizer_model_path='/content/drive/MyDrive/LLM/configs/sentencepiece.model'\n","    training_data_path='/content/drive/MyDrive/LLM/data/training.txt'\n","    validation_data_path='/content/drive/MyDrive/LLM/data/validation.txt'\n","\n","    # Generation Parameters\n","    gen_temperature: float = 0.8\n","    max_gen_tokens: int = 256\n","    top_k: int = 50\n","    top_p: float = 0.9\n","\n","    def __post_init__(self):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.dtype = torch.bfloat16 if self.device == \"cuda\" else torch.float32\n","        if not isinstance(self.save_dir, Path):\n","            self.save_dir = Path(self.save_dir)\n","        self.save_dir.mkdir(parents=True, exist_ok=True)\n","\n","    def print_params(self) -> None:\n","        \"\"\"Print model and training parameters\"\"\"\n","        print(\"Model Parameters:\")\n","        for key, value in self.__dict__.items():\n","            print(f\"  {key}: {value}\")\n","\n","    def save(self, path: Union[str, Path]) -> None:\n","        \"\"\"Save configuration to JSON\"\"\"\n","        path = Path(path)\n","        config_dict = {\n","            k: str(v) if isinstance(v, (Path, torch.dtype)) else v\n","            for k, v in self.__dict__.items()\n","        }\n","        path.parent.mkdir(parents=True, exist_ok=True)\n","        with path.open('w') as f:\n","            json.dump(config_dict, f, indent=4)\n","\n","    @classmethod\n","    def load(cls, path: Union[str, Path]) -> 'ModelConfig':\n","        \"\"\"Load configuration from JSON\"\"\"\n","        path = Path(path)\n","        with path.open('r') as f:\n","            config_dict = json.load(f)\n","\n","        if 'dtype' in config_dict:\n","            config_dict['dtype'] = getattr(torch, config_dict['dtype'].split('.')[-1])\n","        if 'save_dir' in config_dict:\n","            config_dict['save_dir'] = str(config_dict['save_dir'])\n","\n","        return cls(**config_dict)"]},{"cell_type":"code","source":["\n","'''\n","from pathlib import Path\n","import sentencepiece as spm\n","import math\n","import os\n","from typing import List, Tuple\n","import random\n","\n","class VocabularyOptimizer(ModelConfig):\n","    def __init__(self, data_path: Path, max_candidates=5):\n","        self.data_path = data_path\n","        self.max_candidates = max_candidates\n","        self.dataset_stats = self._analyze_dataset()\n","\n","    def _analyze_dataset(self) -> dict:\n","        \"\"\"Analyze dataset characteristics to inform vocab size selection\"\"\"\n","        with open(self.data_path, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","\n","        return {\n","            'size_mb': os.path.getsize(self.data_path) / (1024 ** 2),\n","            'total_chars': len(text),\n","            'unique_chars': len(set(text)),\n","            'avg_word_length': self._calculate_avg_word_length(text)\n","        }\n","\n","    def _calculate_avg_word_length(self, text: str) -> float:\n","        \"\"\"Calculate average word length for whitespace tokenization\"\"\"\n","        words = text.split()\n","        return sum(len(word) for word in words) / len(words) if words else 0\n","\n","    def _generate_vocab_candidates(self) -> List[int]:\n","        \"\"\"Generate candidate vocab sizes based on dataset characteristics\"\"\"\n","        base_size = max(\n","            self.dataset_stats['unique_chars'] * 2,  # Minimum coverage\n","            int(self.dataset_stats['total_chars'] ** 0.5)  # Square root heuristic\n","        )\n","        return sorted({\n","            base_size,\n","            base_size * 2,\n","            base_size * 4,\n","            int(2 ** (math.log2(base_size) + 1)),\n","            min(32000, base_size * 8)  # Upper bound\n","        })\n","\n","    def _evaluate_vocab_size(self, vocab_size: int) -> Tuple[float, float]:\n","        \"\"\"Train and evaluate a single vocab size using perplexity\"\"\"\n","        model_prefix = f\"temp_model_{vocab_size}\"\n","\n","        # Train SentencePiece model\n","        spm.SentencePieceTrainer.train(\n","            input=str(self.data_path),\n","            model_prefix=model_prefix,\n","            vocab_size=vocab_size,\n","            character_coverage=1.0,\n","            model_type='bpe',\n","            num_threads=os.cpu_count()\n","        )\n","\n","        # Evaluate model quality\n","        sp_model = spm.SentencePieceProcessor()\n","        sp_model.load(f\"{model_prefix}.model\")\n","\n","        # Calculate compression ratio and entropy\n","        encoded = sp_model.encode_as_ids(open(self.data_path).read())\n","        entropy = self._calculate_entropy(encoded, vocab_size)\n","        compression_ratio = len(encoded) / self.dataset_stats['total_chars']\n","\n","        # Cleanup temporary model\n","        os.remove(f\"{model_prefix}.model\")\n","        os.remove(f\"{model_prefix}.vocab\")\n","\n","        return entropy, compression_ratio\n","\n","    def _calculate_entropy(self, token_ids: List[int], vocab_size: int) -> float:\n","        \"\"\"Calculate empirical entropy of token distribution\"\"\"\n","        from collections import Counter\n","        counts = Counter(token_ids)\n","        total = len(token_ids)\n","        return -sum((count/total) * math.log2(count/total)\n","                  for count in counts.values() if count > 0)\n","\n","    def optimize_vocab_size(self) -> int:\n","        \"\"\"Main optimization pipeline with early stopping\"\"\"\n","        candidates = self._generate_vocab_candidates()[:self.max_candidates]\n","        best_size = None\n","        best_score = float('inf')\n","        results = []\n","\n","        for vocab_size in sorted(candidates):\n","            try:\n","                entropy, compression = self._evaluate_vocab_size(vocab_size)\n","                # Combined score weights entropy 70% and compression 30%\n","                score = 0.7 * entropy + 0.3 * compression\n","\n","                results.append((vocab_size, score, entropy, compression))\n","\n","                if score < best_score:\n","                    best_score = score\n","                    best_size = vocab_size\n","                else:\n","                    # Early stopping if performance plateaus\n","                    break\n","\n","            except Exception as e:\n","                print(f\"Failed training with vocab_size {vocab_size}: {str(e)}\")\n","                continue\n","\n","        # Print formatted results table\n","        print(\"Vocab Size\\tTotal Score\\tEntropy\\tCompression Ratio\")\n","        for result in results:\n","            print(f\"{result[0]}\\t{result[1]:.4f}\\t{result[2]:.4f}\\t{result[3]:.4f}\")\n","\n","        return best_size\n","\n","# Usage example\n","if __name__ == \"__main__\":\n","    optimizer = VocabularyOptimizer(Path(ModelConfig.training_data_path))\n","    optimal_vocab_size = optimizer.optimize_vocab_size()\n","    print(f\"Recommended vocabulary size: {optimal_vocab_size}\")\n","\n","    '''"],"metadata":{"id":"Rh_KcYXkwHKt"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qp4ZWu0UjQBh"},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","from typing import Union\n","import sentencepiece as spm\n","\n","\n","class TokenDataset(Dataset):\n","    \"\"\"Dataset for tokenized text sequences.\"\"\"\n","    def __init__(self, tokens: list, block_size: int):\n","        self.tokens = tokens\n","        self.block_size = block_size\n","\n","    def __len__(self):\n","        return len(self.tokens) - self.block_size\n","\n","    def __getitem__(self, idx: int):\n","        x = self.tokens[idx : idx + self.block_size]\n","        y = self.tokens[idx + 1 : idx + self.block_size + 1]\n","        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n","\n","class DataManager:\n","    def __init__(self, config: ModelConfig):\n","        self.config = config\n","        self._tokenizer_initialized = False\n","        self.tokenizer = None\n","        self.train_loader = None\n","        self.val_loader = None\n","\n","    def initialize_tokenizer(self):\n","        \"\"\"Initialize SentencePiece tokenizer from training data.\"\"\"\n","        try:\n","            model_dir = Path(self.config.tokenizer_model_path).parent\n","            model_dir.mkdir(parents=True, exist_ok=True)\n","            model_prefix = model_dir / 'sentencepiece'\n","\n","            spm.SentencePieceTrainer.train(\n","                input=str(self.config.training_data_path),\n","                model_prefix=str(model_prefix),\n","                vocab_size=self.config.vocab_size,\n","                character_coverage=1.0,\n","                model_type='bpe'\n","            )\n","\n","            self.tokenizer = spm.SentencePieceProcessor()\n","            self.tokenizer.load(str(model_prefix) + '.model')\n","\n","            self.char_to_idx = {self.tokenizer.id_to_piece(i): i for i in range(self.tokenizer.get_piece_size())}\n","            self.idx_to_char = {i: self.tokenizer.id_to_piece(i) for i in range(self.tokenizer.get_piece_size())}\n","\n","            self._tokenizer_initialized = True\n","\n","        except Exception as e:\n","            print(f\"Error initializing tokenizer: {str(e)}\")\n","            raise\n","\n","    def _load_and_tokenize(self, file_path: Union[str, Path]) -> list:\n","        \"\"\"Tokenize a text file into a list of token IDs.\"\"\"\n","        tokens = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if not line.strip():\n","                    continue\n","                line_tokens = self.tokenizer.encode(line.strip(), out_type=int)\n","                tokens.extend(line_tokens)\n","        return tokens\n","\n","    def load_data(self):\n","        \"\"\"Load and tokenize training/validation data into DataLoaders.\"\"\"\n","        if not self._tokenizer_initialized:\n","            self.initialize_tokenizer()\n","\n","        # Load training data\n","        train_tokens = self._load_and_tokenize(self.config.training_data_path)\n","        self.train_dataset = TokenDataset(train_tokens, self.config.block_size)\n","        self.train_loader = DataLoader(\n","            self.train_dataset,\n","            batch_size=self.config.batch_size,\n","            shuffle=True,\n","            num_workers=self.config.num_workers,\n","            pin_memory=self.config.pin_memory,\n","            prefetch_factor=self.config.prefetch_factor\n","        )\n","\n","        # Load validation data\n","        val_tokens = self._load_and_tokenize(self.config.validation_data_path)\n","        self.val_dataset = TokenDataset(val_tokens, self.config.block_size)\n","        self.val_loader = DataLoader(\n","            self.val_dataset,\n","            batch_size=self.config.batch_size,\n","            shuffle=False,\n","            num_workers=self.config.num_workers,\n","            pin_memory=self.config.pin_memory,\n","            prefetch_factor=self.config.prefetch_factor\n","        )\n","\n","    def cleanup(self):\n","        \"\"\"Cleanup resources and memory.\"\"\"\n","        for attr in ['train_loader', 'val_loader', 'train_dataset', 'val_dataset', 'tokenizer']:\n","            if hasattr(self, attr):\n","                delattr(self, attr)\n","        torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7IlwjJai79ni"},"outputs":[],"source":["class RotaryEmbedding(nn.Module):\n","    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n","        super().__init__()\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer('inv_freq', inv_freq)\n","        self.max_seq_len_cached = max_position_embeddings\n","\n","        # Initialize cache\n","        t = torch.arange(max_position_embeddings, device=inv_freq.device).type_as(inv_freq)\n","        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.cos_cached = emb.cos()[None, None, :, :]\n","        self.sin_cached = emb.sin()[None, None, :, :]\n","\n","    def forward(self, x, seq_len=None):\n","        if seq_len > self.max_seq_len_cached:\n","            self.max_seq_len_cached = seq_len\n","            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n","            freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","            emb = torch.cat((freqs, freqs), dim=-1)\n","            self.cos_cached = emb.cos()[None, None, :, :]\n","            self.sin_cached = emb.sin()[None, None, :, :]\n","\n","        return (\n","            self.cos_cached[:, :, :seq_len, ...].to(x.device),\n","            self.sin_cached[:, :, :seq_len, ...].to(x.device)\n","        )\n","\n","def rotate_half(x):\n","    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","def apply_rotary_pos_emb(q, k, cos, sin):\n","    q_embed = (q * cos) + (rotate_half(q) * sin)\n","    k_embed = (k * cos) + (rotate_half(k) * sin)\n","    return q_embed, k_embed\n","\n","class CausalSelfAttention(nn.Module):\n","    \"\"\"Multi-head causal self-attention layer with RoPE and residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0\n","\n","        # Store necessary dimensions\n","        self.n_embed = config.n_embed\n","        self.n_head = config.n_head\n","        self.head_dim = config.n_embed // config.n_head\n","\n","        # Key, query, value projections for all heads\n","        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias=config.bias)\n","        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n","\n","        # Regularization\n","        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n","\n","        # Initialize RoPE\n","        self.rope = RotaryEmbedding(\n","            self.head_dim,\n","            max_position_embeddings=config.block_size\n","        )\n","\n","        # Flash attention support\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            self.register_buffer(\n","                \"mask\",\n","                torch.tril(torch.ones(config.block_size, config.block_size))\n","                .view(1, 1, config.block_size, config.block_size)\n","            )\n","\n","        # Scaling factor for attention\n","        self.scale = 1.0 / math.sqrt(self.head_dim)\n","\n","    def forward(self, x):\n","        B, T, C = x.size()\n","\n","        # Calculate query, key, values for all heads\n","        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n","\n","        # Reshape for multi-head attention\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","\n","        # Apply RoPE to queries and keys\n","        cos, sin = self.rope(q, seq_len=T)\n","        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n","\n","        # Causal self-attention with flash attention optimization\n","        if self.flash:\n","            y = torch.nn.functional.scaled_dot_product_attention(\n","                q, k, v,\n","                attn_mask=None,\n","                dropout_p=0.0,  # Disabled in favor of residual dropout\n","                is_causal=True,\n","                scale=self.scale\n","            )\n","        else:\n","            att = (q @ k.transpose(-2, -1)) * self.scale\n","            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            y = att @ v\n","\n","        # Re-assemble and project\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","class LayerNorm(nn.Module):\n","    \"\"\"LayerNorm with optional bias\"\"\"\n","    def __init__(self, ndim, bias=True):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class GPTBlock(nn.Module):\n","    \"\"\"Transformer block with residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln1 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln2 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias),\n","            nn.GELU(),\n","            nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias),\n","            nn.Dropout(config.resid_pdrop)\n","        )\n","\n","        # Training optimizations\n","        self.use_checkpointing = config.use_gradient_checkpointing\n","        self.layer_scale_1 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","        self.layer_scale_2 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","\n","    def forward(self, x):\n","        # Attention block\n","        if self.use_checkpointing and x.requires_grad:\n","            attn_output = torch.utils.checkpoint.checkpoint(self.attn, self.ln1(x))\n","        else:\n","            attn_output = self.attn(self.ln1(x))\n","        x = x + self.layer_scale_1.unsqueeze(0).unsqueeze(0) * attn_output\n","\n","        # MLP block\n","        if self.use_checkpointing and x.requires_grad:\n","            mlp_output = torch.utils.checkpoint.checkpoint(self.mlp, self.ln2(x))\n","        else:\n","            mlp_output = self.mlp(self.ln2(x))\n","        x = x + self.layer_scale_2.unsqueeze(0).unsqueeze(0) * mlp_output\n","\n","        return x\n","\n","class GPT(nn.Module):\n","    \"\"\"GPT-like transformer with residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.transformer = nn.ModuleDict({\n","            'wte': nn.Embedding(config.vocab_size, config.n_embed),\n","            'h': nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)]),\n","            'ln_f': nn.LayerNorm(config.n_embed)\n","        })\n","        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n","\n","        # Init\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n","\n","        print(f\"Parameters: {sum(p.numel() for p in self.parameters()) / 1e6:.2f}M\")\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, LayerNorm):\n","            torch.nn.init.ones_(module.weight)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","\n","        with torch.cuda.amp.autocast(enabled=self.config.use_amp, dtype=torch.bfloat16):\n","            x = self.transformer.wte(idx)\n","            for block in self.transformer.h:\n","                x = block(x)\n","            x = self.transformer.ln_f(x)\n","\n","            logits = self.lm_head(x)\n","\n","            # Fixed loss calculation\n","            if targets is not None:\n","                loss = F.cross_entropy(\n","                    logits.view(-1, logits.size(-1)),\n","                    targets.view(-1)\n","                )\n","            else:\n","                # For generation mode\n","                logits = logits[:, -1, :]\n","                loss = None\n","\n","        return logits, loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eKMrUoOX8EKd"},"outputs":[],"source":["class ModelInterface:\n","    \"\"\"Interface for model operations with comprehensive error handling and state management\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"Initialize the interface with configuration and necessary components\"\"\"\n","        self.config = config\n","        self.original_batch_size = config.batch_size  # Store original batch size\n","        self.model = None\n","        self.data_manager = None\n","        self.scaler = torch.cuda.amp.GradScaler(enabled=getattr(config, 'use_amp', True))\n","        self._is_initialized = False\n","        self.checkpoint_info = None\n","        self._initialize()\n","\n","    def _initialize(self):\n","        \"\"\"Safe initialization with resource management and state verification\"\"\"\n","        try:\n","            # Verify and adjust batch size if needed\n","            if self.config.batch_size != self.original_batch_size:\n","                print(f\"Batch size was modified from {self.original_batch_size} to {self.config.batch_size}\")\n","                self.config.batch_size = self.original_batch_size  # Restore original batch size\n","\n","            # Initialize data manager first\n","            self.data_manager = DataManager(self.config)\n","\n","            # Initialize model with proper device placement\n","            self.model = GPT(self.config)\n","            self.model = self.model.to(self.config.device)\n","\n","            self._is_initialized = True\n","\n","            # Log initialization details\n","            print(f\"Model initialized with {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M parameters\")\n","            print(f\"Using device: {self.config.device}\")\n","            print(f\"Batch size: {self.config.batch_size}\")\n","\n","            # Adjust gradient accumulation based on GPU memory if needed\n","            if self.config.device == \"cuda\":\n","                gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n","                if gpu_mem < 8:  # Only adjust if very limited memory\n","                    self.config.gradient_accumulation_steps = 2\n","                    print(f\"Adjusted gradient accumulation steps to {self.config.gradient_accumulation_steps} due to limited GPU memory\")\n","\n","        except Exception as e:\n","            self.cleanup()\n","            raise RuntimeError(f\"Failed to initialize model interface: {str(e)}\")\n","\n","    def verify_state(self):\n","        \"\"\"Verify that the model and vocabulary are in a valid state\"\"\"\n","        if not self._is_initialized:\n","            raise RuntimeError(\"Model interface not properly initialized\")\n","\n","        if not self.model:\n","            raise RuntimeError(\"Model not loaded\")\n","\n","        if not self.data_manager:\n","            raise RuntimeError(\"Data manager not initialized\")\n","\n","        if not self.data_manager.char_to_idx:\n","            raise RuntimeError(\"Vocabulary mappings not loaded\")\n","\n","        # Verify vocabulary consistency\n","        if self.data_manager.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Vocabulary size mismatch: {self.data_manager.vocab_size} != {self.config.vocab_size}\")\n","\n","        # Verify model vocabulary size matches config\n","        if self.model.config.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Model vocabulary size mismatch: {self.model.config.vocab_size} != {self.config.vocab_size}\")\n","\n","        # Verify batch size hasn't been modified\n","        if self.config.batch_size != self.original_batch_size:\n","            print(f\"Batch size mismatch detected. Restoring original batch size: {self.original_batch_size}\")\n","            self.config.batch_size = self.original_batch_size\n","\n","\n","    def load_model(self, model_path: Union[str, Path]):\n","        \"\"\"Load model with strict config validation and Tiktoken tokenizer checks\"\"\"\n","        try:\n","            # Load checkpoint with device mapping\n","            checkpoint = torch.load(model_path, map_location=self.config.device)\n","\n","            # 1. Update config from checkpoint\n","            self.config.__dict__.update(checkpoint['config'])\n","            self.data_manager.config.__dict__.update(checkpoint['config'])\n","\n","            # 2. Initialize Tiktoken tokenizer\n","            self.data_manager.initialize_tokenizer()\n","\n","            # 4. Initialize model with updated config\n","            self.model = GPT(self.config).to(self.config.device)\n","\n","            # 5. Load weights with architecture validation\n","            self.model.load_state_dict(checkpoint['model_state_dict'])\n","            self.model.eval()\n","\n","            # 6. Force device alignment\n","            self.data_manager.config.device = self.config.device\n","\n","            # 7. Warmup GPU and verify state\n","            with torch.cuda.amp.autocast():\n","                _ = self.model.generate(\n","                    torch.zeros((1,1), dtype=torch.long, device=self.config.device),\n","                    max_new_tokens=1\n","                )\n","\n","            self._is_initialized = True\n","            print(f\"Model loaded successfully on {self.config.device}\")\n","\n","        except Exception as e:\n","            self.cleanup()\n","            raise RuntimeError(f\"Model loading failed: {str(e)}\")\n","\n","    def cleanup(self):\n","        \"\"\"Comprehensive cleanup of resources and memory\"\"\"\n","        try:\n","            if hasattr(self, 'model') and self.model is not None:\n","                self.model.cpu()\n","                del self.model\n","                self.model = None\n","\n","            if hasattr(self, 'data_manager') and self.data_manager is not None:\n","                del self.data_manager\n","                self.data_manager = None\n","\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            gc.collect()\n","\n","            self._is_initialized = False\n","            print(\"Model interface cleaned up successfully\")\n","\n","        except Exception as e:\n","            print(f\"Error during cleanup: {str(e)}\")"]},{"cell_type":"code","source":["class LRScheduler:\n","    \"\"\"Learning rate scheduler with multiple scheduling strategies\"\"\"\n","\n","    def __init__(self, optimizer, config):\n","        self.optimizer = optimizer\n","        self.config = config\n","        self.total_steps = config.epochs * config.batch_size\n","        self.warmup_steps = config.warmup_steps\n","        self.current_step = 0\n","        self.base_lr = config.learning_rate\n","        self.min_lr = config.min_learning_rate\n","\n","        # Select scheduling strategy\n","        if config.lr_schedule == 'cosine_with_warmup':\n","            self.get_lr = self._cosine_with_warmup\n","        elif config.lr_schedule == 'linear_with_warmup':\n","            self.get_lr = self._linear_with_warmup\n","        elif config.lr_schedule == 'step':\n","            self.get_lr = self._step_decay\n","        else:\n","            raise ValueError(f\"Unknown lr schedule: {config.lr_schedule}\")\n","\n","    def _cosine_with_warmup(self):\n","        \"\"\"Cosine annealing with warmup\"\"\"\n","        if self.current_step < self.warmup_steps:\n","            # Linear warmup\n","            return self.base_lr * (self.current_step / max(1, self.warmup_steps))\n","        else:\n","            # Cosine decay\n","            progress = (self.current_step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n","            return self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))\n","\n","    def _linear_with_warmup(self):\n","        \"\"\"Linear decay with warmup\"\"\"\n","        if self.current_step < self.warmup_steps:\n","            # Linear warmup\n","            return self.base_lr * (self.current_step / max(1, self.warmup_steps))\n","        else:\n","            # Linear decay\n","            progress = (self.current_step - self.warmup_steps) / max(1, self.total_steps - self.warmup_steps)\n","            return self.min_lr + (self.base_lr - self.min_lr) * (1 - progress)\n","\n","    def _step_decay(self):\n","        \"\"\"Step decay with warmup\"\"\"\n","        if self.current_step < self.warmup_steps:\n","            return self.base_lr * (self.current_step / max(1, self.warmup_steps))\n","        else:\n","            # Decay learning rate by factor of 0.1 every lr_decay_epochs\n","            decay_factor = 0.1 ** (self.current_step // (self.config.lr_decay_epochs * self.config.batch_size))\n","            return max(self.min_lr, self.base_lr * decay_factor)\n","\n","    def step(self):\n","        \"\"\"Update learning rate\"\"\"\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        return lr\n","\n","    def get_last_lr(self):\n","        \"\"\"Get current learning rate\"\"\"\n","        return self.optimizer.param_groups[0]['lr']\n","\n","    def state_dict(self):\n","        \"\"\"Returns the state of the scheduler as a :class:`dict`.\"\"\"\n","        return {\n","            'current_step': self.current_step,\n","            'base_lr': self.base_lr,\n","            'config': self.config.__dict__\n","        }\n","\n","    def load_state_dict(self, state_dict):\n","        \"\"\"Loads the schedulers state.\"\"\"\n","        self.current_step = state_dict['current_step']\n","        self.base_lr = state_dict['base_lr']\n","        self.config.__dict__.update(state_dict['config'])"],"metadata":{"id":"KTO2ZjHx1D7_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QrL4EjmUU9Vl"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjS4oImYf9Jl"},"outputs":[],"source":["import datetime\n","import torch\n","from typing import Optional, Union\n","from pathlib import Path\n","from tqdm import tqdm\n","\n","class Trainer:\n","    def __init__(self, model: torch.nn.Module, data_manager: DataManager, config: ModelConfig):\n","        self.model = model\n","        self.data_manager = data_manager\n","        self.config = config\n","        self.device = config.device\n","        self.epoch = 0\n","        self.global_step = 0\n","        self.best_val_loss = float('inf')\n","        self.no_improvement_count = 0\n","        self.train_losses = []\n","        self.val_losses = []\n","        self.setup_training()\n","\n","    def setup_training(self):\n","        \"\"\"Initialize training components with enhanced LR scheduling.\"\"\"\n","        # Add criterion initialization\n","        self.criterion = torch.nn.CrossEntropyLoss()\n","\n","        self.optimizer = torch.optim.AdamW(\n","            self.model.parameters(),\n","            lr=self.config.learning_rate,\n","            weight_decay=self.config.weight_decay,\n","            betas=(0.9, 0.95)\n","        )\n","\n","        if self.config.use_amp:\n","            self.scaler = torch.cuda.amp.GradScaler()\n","\n","        # Initialize the custom LR scheduler\n","        self.scheduler = LRScheduler(self.optimizer, self.config)\n","\n","    def save_checkpoint(self, loss: float, is_best: bool = False, custom_path: Path = None):\n","        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","        loss_str = f\"{loss:.4f}\".replace('.', '_')\n","        checkpoint = {\n","            'epoch': self.epoch,\n","            'model_state_dict': self.model.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            'criterion_state_dict': self.criterion.state_dict(),\n","            'scheduler_state_dict': {\n","                'current_step': self.scheduler.current_step,\n","                'base_lr': self.scheduler.base_lr,\n","                'config': self.scheduler.config.__dict__\n","            },\n","            'loss': loss,\n","            'global_step': self.global_step,\n","            'best_val_loss': self.best_val_loss,\n","            'train_losses': self.train_losses,\n","            'val_losses': self.val_losses,\n","            'config': {\n","                k: str(v) if isinstance(v, (Path, torch.dtype)) else v\n","                for k, v in self.config.__dict__.items()\n","            }  # Save the full config as a dictionary\n","        }\n","\n","        checkpoint_dir = Path('/content/drive/MyDrive/LLM/checkpoints') if is_colab() else Path('./LLM/checkpoints')\n","        checkpoint_dir.mkdir(parents=True, exist_ok=True)\n","\n","        if custom_path is None:\n","            checkpoint_name = f\"{'best_model' if is_best else 'checkpoint'}_epoch_{self.epoch + 1}_loss_{loss_str}_{timestamp}.pt\"\n","            checkpoint_path = checkpoint_dir / checkpoint_name\n","        else:\n","            checkpoint_path = custom_path\n","\n","        torch.save(checkpoint, str(checkpoint_path))\n","        print(f\"Checkpoint saved: {checkpoint_path}\")\n","\n","    def _validate_training_setup(self):\n","        if not self.data_manager.tokenizer:\n","            raise RuntimeError(\"Tokenizer not initialized\")\n","\n","    def train(self):\n","        \"\"\"Train the model for the specified number of epochs.\"\"\"\n","        self._validate_training_setup()\n","\n","        for epoch in range(self.config.epochs):\n","            self.epoch = epoch\n","            train_loss = self.train_epoch()\n","            val_loss = self.evaluate()\n","\n","            # Track losses\n","            self.train_losses.append(train_loss)\n","            self.val_losses.append(val_loss)\n","\n","            # Early stopping check\n","            if val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.no_improvement_count = 0\n","                self.save_checkpoint(val_loss, is_best=True)\n","            else:\n","                self.no_improvement_count += 1\n","\n","            if self.no_improvement_count >= self.config.patience:\n","                print(f\"Early stopping triggered after {epoch + 1} epochs\")\n","                break\n","\n","            print(f\"Epoch {epoch + 1}/{self.config.epochs} | \"\n","                  f\"Train Loss: {train_loss:.4f} | \"\n","                  f\"Val Loss: {val_loss:.4f} | \"\n","                  f\"LR: {self.scheduler.get_last_lr():.2e}\")\n","\n","    def train_epoch(self):\n","        self.model.train()\n","        total_loss = 0\n","        start_time = time.time()\n","        total_batches = len(self.data_manager.train_loader)\n","\n","        def _format_time(seconds):\n","            \"\"\"Convert seconds to mm:ss format\"\"\"\n","            minutes = int(seconds) // 60\n","            seconds = int(seconds) % 60\n","            return f\"{minutes:02d}:{seconds:02d}\"\n","\n","        for batch_idx, (input_ids, targets) in enumerate(self.data_manager.train_loader):\n","            # Move data to device\n","            input_ids = input_ids.to(self.config.device)\n","            targets = targets.to(self.config.device)\n","\n","            # Forward pass\n","            with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n","                logits, loss = self.model(input_ids, targets=targets)\n","\n","            # Scale loss if using gradient accumulation\n","            if self.config.gradient_accumulation_steps > 1:\n","                loss = loss / self.config.gradient_accumulation_steps\n","\n","            # Backward pass\n","            self.scaler.scale(loss).backward()\n","\n","            # Step if gradient accumulation criteria met\n","            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:\n","                self.scaler.unscale_(self.optimizer)\n","                torch.nn.utils.clip_grad_norm_(\n","                    self.model.parameters(),\n","                    self.config.gradient_clip_val\n","                )\n","                self.scaler.step(self.optimizer)\n","                self.scaler.update()\n","                self.optimizer.zero_grad(set_to_none=True)\n","\n","                if self.scheduler is not None:\n","                    self.scheduler.step()\n","\n","            # Update metrics\n","            total_loss += loss.item()\n","\n","            # Get learning rate (handling both list and float return types)\n","            if hasattr(self.scheduler, 'get_last_lr'):\n","                last_lr = self.scheduler.get_last_lr()\n","                current_lr = last_lr[0] if isinstance(last_lr, list) else last_lr\n","            else:\n","                current_lr = self.optimizer.param_groups[0]['lr']\n","\n","            avg_loss = total_loss / (batch_idx + 1)\n","\n","            # Calculate progress and timing\n","            progress = (batch_idx + 1) / total_batches\n","            elapsed_time = time.time() - start_time\n","            remaining_time = elapsed_time / progress - elapsed_time if progress > 0 else 0\n","\n","            # Progress bar\n","            bar_length = 20\n","            filled_length = int(bar_length * progress)\n","            bar = '█' * filled_length + '░' * (bar_length - filled_length)\n","\n","            # Status line\n","            status = (\n","                f'\\r│ E{self.epoch+1:02d} │ '\n","                f'{bar} │ '\n","                f'{progress:>3.0%} │ '\n","                f'Loss: {avg_loss:.4f} │ '\n","                f'LR: {current_lr:.2e} │ '\n","                f'{_format_time(elapsed_time)}/{_format_time(remaining_time)} │'\n","            )\n","\n","            print(status, end='', flush=True)\n","\n","        print()  # New line after epoch completion\n","        return total_loss / total_batches\n","\n","    def _training_step(self, x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Single training step with gradient scaling and LR scheduling.\"\"\"\n","        with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n","            _, loss = self.model(x, targets=y)\n","\n","        if self.config.use_amp:\n","            self.scaler.scale(loss).backward()\n","            self.scaler.unscale_(self.optimizer)\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n","            self.scaler.step(self.optimizer)\n","            self.scaler.update()\n","        else:\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n","            self.optimizer.step()\n","\n","        self.optimizer.zero_grad(set_to_none=True)\n","        return loss\n","\n","    @torch.no_grad()\n","    def evaluate(self):\n","        self.model.eval()\n","        total_loss = 0\n","\n","        with torch.no_grad():\n","            for input_ids, targets in self.data_manager.val_loader:\n","                input_ids = input_ids.to(self.config.device)\n","                targets = targets.to(self.config.device)\n","\n","                with torch.cuda.amp.autocast(enabled=self.config.use_amp):\n","                    logits, loss = self.model(input_ids, targets=targets)  # Model returns both logits and loss\n","\n","                total_loss += loss.item()\n","\n","        return total_loss / len(self.data_manager.val_loader)\n","\n","    def load_checkpoint(self, checkpoint_path: Union[str, Path]):\n","        \"\"\"Load checkpoint with complete model configuration.\"\"\"\n","        checkpoint = torch.load(checkpoint_path, map_location=self.device)\n","\n","        # Load model state\n","        self.model.load_state_dict(checkpoint['model_state_dict'])\n","\n","        # Load optimizer state\n","        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","\n","        # Load criterion state if it exists\n","        if 'criterion_state_dict' in checkpoint:\n","            self.criterion.load_state_dict(checkpoint['criterion_state_dict'])\n","\n","        # Load scheduler state\n","        if 'scheduler_state_dict' in checkpoint:\n","            self.scheduler.current_step = checkpoint['scheduler_state_dict']['current_step']\n","            self.scheduler.base_lr = checkpoint['scheduler_state_dict']['base_lr']\n","\n","        # Load training state\n","        self.epoch = checkpoint['epoch']\n","        self.global_step = checkpoint['global_step']\n","        self.best_val_loss = checkpoint['best_val_loss']\n","        self.train_losses = checkpoint['train_losses']\n","        self.val_losses = checkpoint['val_losses']\n","\n","        # Load config if it exists\n","        if 'config' in checkpoint:\n","            # Convert string representations back to proper types if needed\n","            config_dict = checkpoint['config']\n","            if 'dtype' in config_dict:\n","                config_dict['dtype'] = getattr(torch, config_dict['dtype'].split('.')[-1])\n","            if 'amp_dtype' in config_dict:\n","                config_dict['amp_dtype'] = getattr(torch, config_dict['amp_dtype'].split('.')[-1])\n","            if 'save_dir' in config_dict:\n","                config_dict['save_dir'] = Path(config_dict['save_dir'])\n","\n","            # Update the current config with saved values\n","            for key, value in config_dict.items():\n","                if hasattr(self.config, key):\n","                    setattr(self.config, key, value)\n","\n","        print(f\"Checkpoint loaded from: {checkpoint_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WuHyArzcgTBw"},"outputs":[],"source":["class TrainingPipeline:\n","    def __init__(self, model: GPT, config: ModelConfig):\n","        self.model = model\n","        self.config = config\n","        self.best_val_loss = float('inf')\n","\n","        # Setup logging\n","        self.setup_logging()\n","\n","        # Initialize components\n","        self.initialize_pipeline()\n","\n","    def setup_logging(self):\n","        \"\"\"Setup logging formats and headers\"\"\"\n","        self.log_formats = {\n","            'epoch': \"{: >4}/{}\", # epoch/total_epochs\n","            'train_loss': \"{:.4f}\",\n","            'val_loss': \"{:.4f}\",\n","            'lr': \"{:.2e}\",\n","            'time': \"{:.1f}s\",\n","            'eta': \"{}\",\n","            'best': \"✓\" if True else \" \"\n","        }\n","\n","        self.header = (\n","            \"\\n\\033[1m\"  # Bold\n","            f\"{'Epoch':>6} | {'Train Loss':>10} | {'Val Loss':>10} | \"\n","            f\"{'LR':>10} | {'Time':>8} | {'ETA':>12} | {'Best':>4}\"\n","            \"\\033[0m\"  # Reset bold\n","        )\n","\n","        self.divider = \"-\" * 70\n","\n","    def initialize_pipeline(self):\n","        \"\"\"Initialize data manager and trainer with progress tracking\"\"\"\n","        print(\"\\n🔧 Initializing Pipeline:\")\n","        print(self.divider)\n","\n","        # Initialize data manager\n","        print(\"📂 Initializing data manager...\")\n","        self.data_manager = DataManager(self.config)\n","\n","        print(\"📥 Loading data...\")\n","        self.data_manager.load_data()\n","\n","        if self.data_manager.train_loader is None:\n","            raise ValueError(\"❌ Training data failed to load\")\n","\n","        # Calculate warmup steps\n","        if hasattr(self.config, 'warmup_ratio'):\n","            total_steps = len(self.data_manager.train_loader) * self.config.epochs\n","            self.config.warmup_steps = int(total_steps * self.config.warmup_ratio)\n","            print(f\"🔥 Warmup steps: {self.config.warmup_steps:,} ({self.config.warmup_ratio*100:.1f}% of total)\")\n","\n","        print(\"🚀 Initializing trainer...\")\n","        self.trainer = Trainer(self.model, self.data_manager, self.config)\n","\n","        print(self.divider)\n","\n","    def print_training_summary(self):\n","        \"\"\"Print training configuration summary\"\"\"\n","        print(\"\\n📊 Training Configuration:\")\n","        print(self.divider)\n","        print(f\"• Model Parameters: {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M\")\n","        print(f\"• Device: {self.config.device}\")\n","        print(f\"• Batch Size: {self.config.batch_size}\")\n","        print(f\"• Learning Rate: {self.config.learning_rate:.2e}\")\n","        print(f\"• Training Epochs: {self.config.epochs}\")\n","        print(f\"• Gradient Accumulation: {self.config.gradient_accumulation_steps}\")\n","        print(self.divider)\n","\n","    def train_and_evaluate(self):\n","        \"\"\"Main training loop with improved progress tracking\"\"\"\n","        try:\n","            self.print_training_summary()\n","            print(self.header)\n","\n","            start_time = time.time()\n","            epoch_times = []\n","\n","            while self.trainer.epoch < self.config.epochs:\n","                epoch_start = time.time()\n","                current_epoch = self.trainer.epoch\n","\n","                # Training and evaluation\n","                train_loss = self.trainer.train_epoch()\n","                val_loss = self.trainer.evaluate()\n","\n","                # Update best model tracking\n","                is_best = val_loss < self.best_val_loss\n","                if is_best:\n","                    self.best_val_loss = val_loss\n","                    self.trainer.save_checkpoint(val_loss, is_best)\n","\n","                # Calculate timing and ETA\n","                epoch_time = time.time() - epoch_start\n","                epoch_times.append(epoch_time)\n","                avg_epoch_time = np.mean(epoch_times[-5:]) if epoch_times else epoch_time\n","                remaining_epochs = self.config.epochs - (current_epoch + 1)\n","                eta = str(datetime.timedelta(seconds=int(avg_epoch_time * remaining_epochs)))\n","\n","                # Print progress\n","                progress = (\n","                    f\"{current_epoch + 1:>6}/{self.config.epochs} | \"\n","                    f\"{train_loss:>10.4f} | \"\n","                    f\"{val_loss:>10.4f} | \"\n","                    f\"{self.trainer.scheduler.get_last_lr():>10.2e} | \"\n","                    f\"{epoch_time:>8.1f}s | \"\n","                    f\"{eta:>12} | \"\n","                    f\"{'✓' if is_best else ' ':>4}\"\n","                )\n","                print(progress)\n","\n","                self.trainer.epoch += 1\n","\n","                # Early stopping check\n","                if self.trainer.no_improvement_count >= self.config.patience:\n","                    print(f\"\\n⚠️  Early stopping triggered after {current_epoch + 1} epochs\")\n","                    break\n","\n","            # Training summary\n","            total_time = time.time() - start_time\n","            print(self.divider)\n","            print(f\"\\n✨ Training completed in {str(datetime.timedelta(seconds=int(total_time)))}\")\n","            print(f\"🏆 Best validation loss: {self.best_val_loss:.4f}\")\n","\n","        except KeyboardInterrupt:\n","            print(\"\\n\\n⚠️  Training interrupted by user. Saving checkpoint...\")\n","            self.trainer.save_checkpoint(val_loss, is_best=False)\n","\n","        except Exception as e:\n","            print(f\"\\n❌ Error during training: {str(e)}\")\n","            raise\n","\n","        finally:\n","            # Cleanup\n","            torch.cuda.empty_cache()\n","            gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8rpu9PRvfKv"},"outputs":[],"source":["if __name__ == \"__main__\":\n","    try:\n","        # Initialize the configuration with optimized values for small LLM\n","        config = ModelConfig(\n","            # Model Architecture\n","            vocab_size=100,  # Will be updated after data loading\n","            block_size=64,  # Context window size\n","            n_layer=3,        # Reduced from 6\n","            n_head=4,\n","            n_embed=128,      # Reduced from 256\n","            ff_dim=512,       # Reduced from 1024\n","            head_dim=64,      # Reduced from 128\n","\n","            # Increase Regularization\n","            resid_pdrop=0.2,      # Increased from 0.1\n","            weight_decay=0.05,    # Increased from 0.01\n","\n","            # Architecture Features\n","            bias=True,\n","            flash_attn=True,  # New: use flash attention if available\n","            use_gradient_checkpointing=False,  # Memory efficiency\n","\n","            # Training Parameters\n","            batch_size=128,\n","            gradient_accumulation_steps=1,\n","            epochs=300,\n","            gradient_clip_val=1.0,\n","\n","            # Learning Rate - More conservative\n","            learning_rate=1e-4,   # Reduced from 1e-4\n","            min_learning_rate=5e-6,  # Adjusted proportionally\n","            warmup_ratio=0.0002,    # Increased from 0.01\n","            lr_schedule='cosine_with_warmup',\n","            lr_decay_epochs=9,    # Reduced from 8\n","\n","            # Early Stopping - More aggressive\n","            loss_threshold=0.3,\n","            within_epoch_loss_threshold=0.3,\n","            patience=5,           # Reduced from 100 - much more aggressive\n","            min_delta=1e-3,      # Increased from 1e-4\n","\n","            # Evaluation and Checkpointing\n","            eval_steps=500,       # Reduced from 250\n","            eval_every=1000,       # Reduced from 500\n","            save_every=5,\n","            keep_last_n_checkpoints=2,\n","\n","            # Generation Parameters\n","            gen_temperature=0.8,\n","            max_gen_tokens=64,\n","            top_k=50,\n","            top_p=0.9,        # New: nucleus sampling parameter\n","\n","            # System and Memory\n","            device='cuda' if torch.cuda.is_available() else 'cpu',\n","            pin_memory=True,\n","            num_workers=8,\n","            prefetch_factor=2,\n","\n","            # Precision\n","            use_amp=True,\n","            amp_dtype=torch.bfloat16,\n","            dtype=torch.bfloat16\n","        )\n","\n","        # Initialize data pipeline with error handling\n","        print(\"Initializing data manager...\")\n","        data_manager = DataManager(config)\n","\n","        # Load and process data\n","        print(\"Loading and processing data...\")\n","        data_manager.load_data()\n","\n","        # Analyze vocabulary\n","        unique_characters = list(data_manager.char_to_idx.keys())\n","        print(f\"Found {len(unique_characters)} unique characters\")\n","\n","        # Initialize model with memory optimization\n","        print(\"Initializing model...\")\n","        with torch.cuda.amp.autocast(enabled=config.use_amp):\n","            model = GPT(config).to(config.device)\n","\n","        # Log model parameters\n","        total_params = sum(p.numel() for p in model.parameters())\n","        print(f\"Model initialized with {total_params/1e6:.2f}M parameters\")\n","\n","        # Initialize and run training pipeline with error handling\n","        print(\"Initializing training pipeline...\")\n","        pipeline = TrainingPipeline(model, config)\n","\n","        # Run training with resource monitoring\n","        print(\"Starting training...\")\n","        try:\n","            pipeline.train_and_evaluate()\n","        except KeyboardInterrupt:\n","            print(\"Training interrupted by user. Saving checkpoint...\")\n","            pipeline.trainer.save_checkpoint(\n","                pipeline.trainer.evaluate(),\n","                is_best=False,\n","                custom_path=Path(config.save_dir) / \"interrupted_checkpoint.pt\"\n","            )\n","\n","        print(\"Training completed successfully\")\n","\n","    except Exception as e:\n","        print(f\"Error during execution: {str(e)}\")\n","        raise\n","    finally:\n","        # Cleanup\n","        if torch.cuda.is_available():\n","            torch.cuda.empty_cache()\n","        gc.collect()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1U8KnhL_CCAv8ZYhRDuM6HpY8wM5VKUNC","timestamp":1738074740202},{"file_id":"1XOBCh-gyKZcRjkhuVdAjZlAeoBWfCZ47","timestamp":1736534076432},{"file_id":"1zHaOvxWM8twT04dcIaplOJUgz_hCQ1BH","timestamp":1736024230358},{"file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","timestamp":1735856996491}],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}