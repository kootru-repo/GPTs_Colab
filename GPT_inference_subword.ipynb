{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"qqqmXrJxt-XO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738355069630,"user_tz":360,"elapsed":40603,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"99e2da45-9b21-4e32-a778-e6d79a26a5cf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Collecting pynvml\n","  Downloading pynvml-12.0.0-py3-none-any.whl.metadata (5.4 kB)\n","Collecting nvidia_ml_py3\n","  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting gputil\n","  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting fastapi\n","  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n","Collecting uvicorn\n","  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n","Collecting nvidia-ml-py<13.0.0a0,>=12.0.0 (from pynvml)\n","  Downloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\n","Collecting starlette<0.46.0,>=0.40.0 (from fastapi)\n","  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.10.6)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.12.2)\n","Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.1.8)\n","Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.14.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.2)\n","Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.46.0,>=0.40.0->fastapi) (3.7.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (3.10)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi) (1.3.1)\n","Downloading pynvml-12.0.0-py3-none-any.whl (26 kB)\n","Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading starlette-0.45.3-py3-none-any.whl (71 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: nvidia_ml_py3, gputil\n","  Building wheel for nvidia_ml_py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for nvidia_ml_py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=4938c326d30de5ac0f45caea9cbedff52e426edefe49b78adb24b7213b38edcf\n","  Stored in directory: /root/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\n","  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=a8452f797751c103df0cc0274dbb978188455cc44636b32ee9302569a61c24d4\n","  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n","Successfully built nvidia_ml_py3 gputil\n","Installing collected packages: nvidia_ml_py3, nvidia-ml-py, gputil, uvicorn, pynvml, starlette, fastapi\n","Successfully installed fastapi-0.115.8 gputil-1.4.0 nvidia-ml-py-12.570.86 nvidia_ml_py3-7.352.0 pynvml-12.0.0 starlette-0.45.3 uvicorn-0.34.0\n","Section 1: Initial setup and core components complete\n"]}],"source":["import os\n","import sys\n","import io\n","import time\n","import random\n","import json\n","import gc\n","import warnings\n","from pathlib import Path\n","from datetime import timedelta\n","import psutil\n","import numpy as np\n","from contextlib import contextmanager, nullcontext\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm import tqdm  # or tqdm.auto if needed\n","from typing import List, Union, Optional, Dict, Any\n","from dataclasses import dataclass\n","import threading\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math\n","import tempfile\n","\n","\n","# Check if running in Google Colab\n","def is_colab():\n","    try:\n","        return 'google.colab' in str(get_ipython())\n","    except NameError:\n","        return False\n","\n","# Mount Google Drive if in Colab\n","if is_colab():\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","# Create LLM directory in Drive\n","base_dir = Path('/content/drive/MyDrive/LLM') if is_colab() else Path('./LLM')\n","for dir_name in ['checkpoints', 'models', 'logs', 'configs', 'data']:\n","    (base_dir / dir_name).mkdir(parents=True, exist_ok=True)\n","\n","def is_package_installed(package_name):\n","    try:\n","        __import__(package_name)\n","        return True\n","    except ImportError:\n","        return False\n","\n","if is_colab():\n","    # PyTorch packages\n","    pytorch_packages = ['torch', 'torchvision', 'torchaudio']\n","    pytorch_install = [pkg for pkg in pytorch_packages if not is_package_installed(pkg)]\n","    if pytorch_install:\n","        !pip install {' '.join(pytorch_install)}\n","\n","    # Additional packages\n","    other_packages = ['pynvml', 'nvidia_ml_py3', 'gputil', 'fastapi', 'uvicorn', 'pydantic', 'sentencepiece']\n","    other_install = [pkg for pkg in other_packages if not is_package_installed(pkg)]\n","    if other_install:\n","        !pip install {' '.join(other_install)}\n","\n","# Suppress warnings\n","warnings.filterwarnings('ignore')\n","\n","print(\"Section 1: Initial setup and core components complete\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"s-hhOXFb4txj","executionInfo":{"status":"ok","timestamp":1738355069630,"user_tz":360,"elapsed":2,"user":{"displayName":"Andrew","userId":"07330014611798335629"}}},"outputs":[],"source":["@dataclass\n","class ModelConfig:\n","    \"\"\"Configuration for model training and architecture\"\"\"\n","    # Model Architecture\n","    vocab_size: int = 0\n","    block_size: int = 64\n","    n_embed: int = 64\n","    n_head: int = 4\n","    n_layer: int = 4\n","    ff_dim: int = 256\n","    head_dim: int = 32\n","\n","    # Regularization\n","    resid_pdrop: float = 0.2\n","    weight_decay: float = 0.1\n","\n","    # Architecture Features\n","    bias: bool = True\n","    flash_attn: bool = True\n","    use_gradient_checkpointing: bool = False\n","\n","    # Training Parameters\n","    batch_size: int = 32\n","    gradient_accumulation_steps: int = 1\n","    epochs: int = 10\n","    max_grad_norm: float = 1.0\n","    gradient_clip_val: float = 1.0\n","    log_interval: int = 100  # Added this parameter\n","\n","    # Enhanced Learning Rate Parameters\n","    learning_rate: float = 1e-4\n","    min_learning_rate: float = 1e-5\n","    warmup_steps: int = 200\n","    lr_schedule: str = 'cosine_with_warmup'  # Options: 'cosine_with_warmup', 'linear_with_warmup', 'step'\n","    lr_decay_epochs: int = 8  # For step scheduler\n","    warmup_ratio: float = 0.001  # Alternative to warmup_steps (ratio of total training steps)\n","\n","    # Early Stopping\n","    patience: int = 5\n","    min_delta: float = 1e-4\n","    loss_threshold: float = 1.0\n","    within_epoch_loss_threshold: float = 0.3\n","\n","    # Evaluation and Checkpointing\n","    eval_steps: int = 10000\n","    eval_every: int = 10000\n","    save_every: int = 1\n","    keep_last_n_checkpoints: int = 5\n","\n","    # Precision\n","    use_amp: bool = True\n","    amp_dtype: torch.dtype = torch.bfloat16\n","    dtype: torch.dtype = torch.bfloat16\n","\n","    # System Parameters\n","    pin_memory: bool = True\n","    device: str = \"cuda\"\n","    num_workers: int = 8\n","    prefetch_factor: int = 5\n","\n","    # Paths\n","    save_dir: Union[str, Path] = Path(\"/content/drive/MyDrive/LLM/checkpoints\")\n","    tokenizer_model_path='/content/drive/MyDrive/LLM/configs/sentencepiece.model'\n","    training_data_path='/content/drive/MyDrive/LLM/data/training.txt'\n","    validation_data_path='/content/drive/MyDrive/LLM/data/validation.txt'\n","\n","    # Generation Parameters\n","    gen_temperature: float = 0.8\n","    max_gen_tokens: int = 256\n","    top_k: int = 50\n","    top_p: float = 0.9\n","\n","    def __post_init__(self):\n","        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","        self.dtype = torch.bfloat16 if self.device == \"cuda\" else torch.float32\n","        if not isinstance(self.save_dir, Path):\n","            self.save_dir = Path(self.save_dir)\n","        self.save_dir.mkdir(parents=True, exist_ok=True)\n","\n","    def print_params(self) -> None:\n","        \"\"\"Print model and training parameters\"\"\"\n","        print(\"Model Parameters:\")\n","        for key, value in self.__dict__.items():\n","            print(f\"  {key}: {value}\")\n","\n","    def save(self, path: Union[str, Path]) -> None:\n","        \"\"\"Save configuration to JSON\"\"\"\n","        path = Path(path)\n","        config_dict = {\n","            k: str(v) if isinstance(v, (Path, torch.dtype)) else v\n","            for k, v in self.__dict__.items()\n","        }\n","        path.parent.mkdir(parents=True, exist_ok=True)\n","        with path.open('w') as f:\n","            json.dump(config_dict, f, indent=4)\n","\n","    @classmethod\n","    def load(cls, path: Union[str, Path]) -> 'ModelConfig':\n","        \"\"\"Load configuration from JSON\"\"\"\n","        path = Path(path)\n","        with path.open('r') as f:\n","            config_dict = json.load(f)\n","\n","        if 'dtype' in config_dict:\n","            config_dict['dtype'] = getattr(torch, config_dict['dtype'].split('.')[-1])\n","        if 'save_dir' in config_dict:\n","            config_dict['save_dir'] = str(config_dict['save_dir'])\n","\n","        return cls(**config_dict)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Qp4ZWu0UjQBh","executionInfo":{"status":"ok","timestamp":1738355069630,"user_tz":360,"elapsed":2,"user":{"displayName":"Andrew","userId":"07330014611798335629"}}},"outputs":[],"source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from pathlib import Path\n","from typing import Union\n","import sentencepiece as spm\n","\n","\n","class TokenDataset(Dataset):\n","    \"\"\"Dataset for tokenized text sequences.\"\"\"\n","    def __init__(self, tokens: list, block_size: int):\n","        self.tokens = tokens\n","        self.block_size = block_size\n","\n","    def __len__(self):\n","        return len(self.tokens) - self.block_size\n","\n","    def __getitem__(self, idx: int):\n","        x = self.tokens[idx : idx + self.block_size]\n","        y = self.tokens[idx + 1 : idx + self.block_size + 1]\n","        return torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n","\n","class DataManager:\n","    def __init__(self, config: ModelConfig):\n","        self.config = config\n","        self._tokenizer_initialized = False\n","        self.tokenizer = None\n","        self.train_loader = None\n","        self.val_loader = None\n","\n","    def initialize_tokenizer(self):\n","        \"\"\"Initialize SentencePiece tokenizer from training data.\"\"\"\n","        try:\n","            model_dir = Path(self.config.tokenizer_model_path).parent\n","            model_dir.mkdir(parents=True, exist_ok=True)\n","            model_prefix = model_dir / 'sentencepiece'\n","\n","            spm.SentencePieceTrainer.train(\n","                input=str(self.config.training_data_path),\n","                model_prefix=str(model_prefix),\n","                vocab_size=self.config.vocab_size,\n","                character_coverage=1.0,\n","                model_type='bpe'\n","            )\n","\n","            self.tokenizer = spm.SentencePieceProcessor()\n","            self.tokenizer.load(str(model_prefix) + '.model')\n","\n","            self.char_to_idx = {self.tokenizer.id_to_piece(i): i for i in range(self.tokenizer.get_piece_size())}\n","            self.idx_to_char = {i: self.tokenizer.id_to_piece(i) for i in range(self.tokenizer.get_piece_size())}\n","\n","            self._tokenizer_initialized = True\n","\n","        except Exception as e:\n","            print(f\"Error initializing tokenizer: {str(e)}\")\n","            raise\n","\n","    def _load_and_tokenize(self, file_path: Union[str, Path]) -> list:\n","        \"\"\"Tokenize a text file into a list of token IDs.\"\"\"\n","        tokens = []\n","        with open(file_path, 'r', encoding='utf-8') as f:\n","            for line in f:\n","                if not line.strip():\n","                    continue\n","                line_tokens = self.tokenizer.encode(line.strip(), out_type=int)\n","                tokens.extend(line_tokens)\n","        return tokens\n","\n","    def load_data(self):\n","        \"\"\"Load and tokenize training/validation data into DataLoaders.\"\"\"\n","        if not self._tokenizer_initialized:\n","            self.initialize_tokenizer()\n","\n","        # Load training data\n","        train_tokens = self._load_and_tokenize(self.config.training_data_path)\n","        self.train_dataset = TokenDataset(train_tokens, self.config.block_size)\n","        self.train_loader = DataLoader(\n","            self.train_dataset,\n","            batch_size=self.config.batch_size,\n","            shuffle=True,\n","            num_workers=self.config.num_workers,\n","            pin_memory=self.config.pin_memory,\n","            prefetch_factor=self.config.prefetch_factor\n","        )\n","\n","        # Load validation data\n","        val_tokens = self._load_and_tokenize(self.config.validation_data_path)\n","        self.val_dataset = TokenDataset(val_tokens, self.config.block_size)\n","        self.val_loader = DataLoader(\n","            self.val_dataset,\n","            batch_size=self.config.batch_size,\n","            shuffle=False,\n","            num_workers=self.config.num_workers,\n","            pin_memory=self.config.pin_memory,\n","            prefetch_factor=self.config.prefetch_factor\n","        )\n","\n","    def cleanup(self):\n","        \"\"\"Cleanup resources and memory.\"\"\"\n","        for attr in ['train_loader', 'val_loader', 'train_dataset', 'val_dataset', 'tokenizer']:\n","            if hasattr(self, attr):\n","                delattr(self, attr)\n","        torch.cuda.empty_cache()\n","\n","    def encode(self, text: str) -> list:\n","        \"\"\"Encode text into token IDs.\"\"\"\n","        if not self._tokenizer_initialized:\n","            raise RuntimeError(\"Tokenizer not initialized\")\n","        return self.tokenizer.encode(text, out_type=int)\n","\n","    def decode(self, token_ids: list) -> str:\n","        \"\"\"Decode token IDs back into a human-readable string.\"\"\"\n","        if not self._tokenizer_initialized:\n","            raise RuntimeError(\"Tokenizer not initialized\")\n","        return self.tokenizer.decode(token_ids, out_type=str)"]},{"cell_type":"code","source":["import sentencepiece as spm\n","\n","# makes segmenter instance and loads the model file (m.model)\n","sp = spm.SentencePieceProcessor()\n","sp.load('/content/drive/MyDrive/LLM/configs/sentencepiece.model')\n","\n","# encode: text => id\n","print(sp.encode_as_pieces('his is a sentence to encode.t'))\n","print(sp.encode_as_ids('This is a sentence to encode.'))\n","\n","# decode: id => text\n","print(sp.decode_pieces(['▁This', '▁is', '▁a', '▁t', 'est']))\n","#print(sp.decode_ids([326, 54, 5, 7366, 21, 3369, 1719, 7961]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cmd5CqxpuQ44","executionInfo":{"status":"ok","timestamp":1738355070429,"user_tz":360,"elapsed":133,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"9d42e6a9-c66a-4943-89e1-f429cfa5d6f3"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["['▁h', 'is', '▁', 'is', '▁a', '▁s', 'en', 't', 'en', 'c', 'e', '▁to', '▁', 'en', 'c', 'o', 'd', 'e', '.', 't']\n","[40, 66, 48, 17, 40, 17, 5, 12, 28, 42, 28, 53, 41, 21, 40, 28, 53, 43, 52, 41, 61]\n","▁This▁is a test\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"7IlwjJai79ni","executionInfo":{"status":"ok","timestamp":1738355070429,"user_tz":360,"elapsed":2,"user":{"displayName":"Andrew","userId":"07330014611798335629"}}},"outputs":[],"source":["class RotaryEmbedding(nn.Module):\n","    def __init__(self, dim, max_position_embeddings=2048, base=10000):\n","        super().__init__()\n","        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n","        self.register_buffer('inv_freq', inv_freq)\n","        self.max_seq_len_cached = max_position_embeddings\n","\n","        # Initialize cache\n","        t = torch.arange(max_position_embeddings, device=inv_freq.device).type_as(inv_freq)\n","        freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","        emb = torch.cat((freqs, freqs), dim=-1)\n","        self.cos_cached = emb.cos()[None, None, :, :]\n","        self.sin_cached = emb.sin()[None, None, :, :]\n","\n","    def forward(self, x, seq_len=None):\n","        if seq_len > self.max_seq_len_cached:\n","            self.max_seq_len_cached = seq_len\n","            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n","            freqs = torch.einsum('i,j->ij', t, self.inv_freq)\n","            emb = torch.cat((freqs, freqs), dim=-1)\n","            self.cos_cached = emb.cos()[None, None, :, :]\n","            self.sin_cached = emb.sin()[None, None, :, :]\n","\n","        return (\n","            self.cos_cached[:, :, :seq_len, ...].to(x.device),\n","            self.sin_cached[:, :, :seq_len, ...].to(x.device)\n","        )\n","\n","def rotate_half(x):\n","    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n","    return torch.cat((-x2, x1), dim=-1)\n","\n","def apply_rotary_pos_emb(q, k, cos, sin):\n","    q_embed = (q * cos) + (rotate_half(q) * sin)\n","    k_embed = (k * cos) + (rotate_half(k) * sin)\n","    return q_embed, k_embed\n","\n","class CausalSelfAttention(nn.Module):\n","    \"\"\"Multi-head causal self-attention layer with RoPE and residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        assert config.n_embed % config.n_head == 0\n","\n","        # Store necessary dimensions\n","        self.n_embed = config.n_embed\n","        self.n_head = config.n_head\n","        self.head_dim = config.n_embed // config.n_head\n","\n","        # Key, query, value projections for all heads\n","        self.c_attn = nn.Linear(config.n_embed, 3 * config.n_embed, bias=config.bias)\n","        self.c_proj = nn.Linear(config.n_embed, config.n_embed, bias=config.bias)\n","\n","        # Regularization\n","        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n","\n","        # Initialize RoPE\n","        self.rope = RotaryEmbedding(\n","            self.head_dim,\n","            max_position_embeddings=config.block_size\n","        )\n","\n","        # Flash attention support\n","        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n","        if not self.flash:\n","            self.register_buffer(\n","                \"mask\",\n","                torch.tril(torch.ones(config.block_size, config.block_size))\n","                .view(1, 1, config.block_size, config.block_size)\n","            )\n","\n","        # Scaling factor for attention\n","        self.scale = 1.0 / math.sqrt(self.head_dim)\n","\n","    def forward(self, x):\n","        B, T, C = x.size()\n","\n","        # Calculate query, key, values for all heads\n","        q, k, v = self.c_attn(x).split(self.n_embed, dim=2)\n","\n","        # Reshape for multi-head attention\n","        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n","\n","        # Apply RoPE to queries and keys\n","        cos, sin = self.rope(q, seq_len=T)\n","        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n","\n","        # Causal self-attention with flash attention optimization\n","        if self.flash:\n","            y = torch.nn.functional.scaled_dot_product_attention(\n","                q, k, v,\n","                attn_mask=None,\n","                dropout_p=0.0,  # Disabled in favor of residual dropout\n","                is_causal=True,\n","                scale=self.scale\n","            )\n","        else:\n","            att = (q @ k.transpose(-2, -1)) * self.scale\n","            att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf'))\n","            att = F.softmax(att, dim=-1)\n","            y = att @ v\n","\n","        # Re-assemble and project\n","        y = y.transpose(1, 2).contiguous().view(B, T, C)\n","        y = self.resid_dropout(self.c_proj(y))\n","        return y\n","\n","class LayerNorm(nn.Module):\n","    \"\"\"LayerNorm with optional bias\"\"\"\n","    def __init__(self, ndim, bias=True):\n","        super().__init__()\n","        self.weight = nn.Parameter(torch.ones(ndim))\n","        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n","\n","    def forward(self, input):\n","        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n","\n","class GPTBlock(nn.Module):\n","    \"\"\"Transformer block with residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.ln1 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.attn = CausalSelfAttention(config)\n","        self.ln2 = LayerNorm(config.n_embed, bias=config.bias)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(config.n_embed, 4 * config.n_embed, bias=config.bias),\n","            nn.GELU(),\n","            nn.Linear(4 * config.n_embed, config.n_embed, bias=config.bias),\n","            nn.Dropout(config.resid_pdrop)\n","        )\n","\n","        # Training optimizations\n","        self.use_checkpointing = config.use_gradient_checkpointing\n","        self.layer_scale_1 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","        self.layer_scale_2 = nn.Parameter(torch.ones(config.n_embed) * 0.1)\n","\n","    def forward(self, x):\n","        # Attention block\n","        if self.use_checkpointing and x.requires_grad:\n","            attn_output = torch.utils.checkpoint.checkpoint(self.attn, self.ln1(x))\n","        else:\n","            attn_output = self.attn(self.ln1(x))\n","        x = x + self.layer_scale_1.unsqueeze(0).unsqueeze(0) * attn_output\n","\n","        # MLP block\n","        if self.use_checkpointing and x.requires_grad:\n","            mlp_output = torch.utils.checkpoint.checkpoint(self.mlp, self.ln2(x))\n","        else:\n","            mlp_output = self.mlp(self.ln2(x))\n","        x = x + self.layer_scale_2.unsqueeze(0).unsqueeze(0) * mlp_output\n","\n","        return x\n","\n","class GPT(nn.Module):\n","    \"\"\"GPT-like transformer with residual dropout\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","        self.transformer = nn.ModuleDict({\n","            'wte': nn.Embedding(config.vocab_size, config.n_embed),\n","            'h': nn.ModuleList([GPTBlock(config) for _ in range(config.n_layer)]),\n","            'ln_f': nn.LayerNorm(config.n_embed)\n","        })\n","        self.lm_head = nn.Linear(config.n_embed, config.vocab_size, bias=False)\n","\n","        # Init\n","        self.apply(self._init_weights)\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('c_proj.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n","\n","        print(f\"Parameters: {sum(p.numel() for p in self.parameters()) / 1e6:.2f}M\")\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if isinstance(module, nn.Linear) and module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, LayerNorm):\n","            torch.nn.init.ones_(module.weight)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","\n","    def forward(self, idx, targets=None):\n","        device = idx.device\n","        b, t = idx.size()\n","\n","        with torch.cuda.amp.autocast(enabled=self.config.use_amp, dtype=torch.bfloat16):\n","            x = self.transformer.wte(idx)\n","            for block in self.transformer.h:\n","                x = block(x)\n","            x = self.transformer.ln_f(x)\n","\n","            logits = self.lm_head(x)\n","\n","            # Fixed loss calculation\n","            if targets is not None:\n","                loss = F.cross_entropy(\n","                    logits.view(-1, logits.size(-1)),\n","                    targets.view(-1)\n","                )\n","            else:\n","                # For generation mode\n","                logits = logits[:, -1, :]\n","                loss = None\n","\n","        return logits, loss\n","\n","    @torch.no_grad()\n","    def generate(self, idx: torch.Tensor, max_new_tokens: int,\n","                temperature: float = 1.0, top_k: Optional[int] = None,\n","                top_p: Optional[float] = None) -> torch.Tensor:\n","        \"\"\"Generate tokens with shape validation and top-p sampling\"\"\"\n","        if idx.dim() != 2:\n","            idx = idx.unsqueeze(0)  # Add batch dimension if not present\n","\n","        for _ in range(max_new_tokens):\n","            # Crop sequence if needed\n","            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n","\n","            # Forward pass\n","            logits, _ = self(idx_cond)\n","\n","            # If logits has 3 dimensions (batch, sequence, vocab), take last token\n","            if logits.dim() == 3:\n","                logits = logits[:, -1, :]\n","\n","            # Apply temperature\n","            logits = logits / temperature\n","\n","            # Apply top-k sampling if specified\n","            if top_k is not None:\n","                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n","                logits[logits < v[:, [-1]]] = float('-inf')\n","\n","            # Apply top-p (nucleus) sampling if specified\n","            if top_p is not None:\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n","\n","                # Remove tokens with cumulative probability above the threshold\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                # Shift the indices to the right to keep also the first token above the threshold\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","\n","                indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n","                logits[indices_to_remove] = float('-inf')\n","\n","            # Sample next token\n","            probs = F.softmax(logits, dim=-1)\n","            next_token = torch.multinomial(probs, num_samples=1)\n","\n","            # Append next token\n","            idx = torch.cat((idx, next_token), dim=1)\n","\n","        return idx"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"eKMrUoOX8EKd","executionInfo":{"status":"ok","timestamp":1738355070429,"user_tz":360,"elapsed":2,"user":{"displayName":"Andrew","userId":"07330014611798335629"}}},"outputs":[],"source":["class ModelInterface:\n","    \"\"\"Interface for model operations with comprehensive error handling and state management\"\"\"\n","\n","    def __init__(self, config: ModelConfig):\n","        \"\"\"Initialize the interface with configuration and necessary components\"\"\"\n","        self.config = config\n","        self.original_batch_size = config.batch_size  # Store original batch size\n","        self.model = None\n","        self.data_manager = None\n","        self.scaler = torch.cuda.amp.GradScaler(enabled=getattr(config, 'use_amp', True))\n","        self._is_initialized = False\n","        self.checkpoint_info = None\n","        self._initialize()\n","\n","    def _initialize(self):\n","        \"\"\"Safe initialization with resource management and state verification\"\"\"\n","        try:\n","            # Verify and adjust batch size if needed\n","            if self.config.batch_size != self.original_batch_size:\n","                print(f\"Batch size was modified from {self.original_batch_size} to {self.config.batch_size}\")\n","                self.config.batch_size = self.original_batch_size  # Restore original batch size\n","\n","            # Initialize data manager first\n","            self.data_manager = DataManager(self.config)\n","\n","            # Initialize model with proper device placement\n","            self.model = GPT(self.config)\n","            self.model = self.model.to(self.config.device)\n","\n","            self._is_initialized = True\n","\n","            # Log initialization details\n","            print(f\"Model initialized with {sum(p.numel() for p in self.model.parameters())/1e6:.2f}M parameters\")\n","            print(f\"Using device: {self.config.device}\")\n","            print(f\"Batch size: {self.config.batch_size}\")\n","\n","            # Adjust gradient accumulation based on GPU memory if needed\n","            if self.config.device == \"cuda\":\n","                gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1e9\n","                if gpu_mem < 8:  # Only adjust if very limited memory\n","                    self.config.gradient_accumulation_steps = 2\n","                    print(f\"Adjusted gradient accumulation steps to {self.config.gradient_accumulation_steps} due to limited GPU memory\")\n","\n","        except Exception as e:\n","            self.cleanup()\n","            raise RuntimeError(f\"Failed to initialize model interface: {str(e)}\")\n","\n","    def verify_state(self):\n","        \"\"\"Verify that the model and vocabulary are in a valid state\"\"\"\n","        if not self._is_initialized:\n","            raise RuntimeError(\"Model interface not properly initialized\")\n","\n","        if not self.model:\n","            raise RuntimeError(\"Model not loaded\")\n","\n","        if not self.data_manager:\n","            raise RuntimeError(\"Data manager not initialized\")\n","\n","        if not self.data_manager.char_to_idx:\n","            raise RuntimeError(\"Vocabulary mappings not loaded\")\n","\n","        # Verify model vocabulary size matches config\n","        if self.model.config.vocab_size != self.config.vocab_size:\n","            raise ValueError(f\"Model vocabulary size mismatch: {self.model.config.vocab_size} != {self.config.vocab_size}\")\n","\n","        # Verify batch size hasn't been modified\n","        if self.config.batch_size != self.original_batch_size:\n","            print(f\"Batch size mismatch detected. Restoring original batch size: {self.original_batch_size}\")\n","            self.config.batch_size = self.original_batch_size\n","\n","\n","    def load_model(self, model_path: Union[str, Path]):\n","        \"\"\"Load model with strict config validation and Tiktoken tokenizer checks\"\"\"\n","        try:\n","            # Load checkpoint with device mapping\n","            checkpoint = torch.load(model_path, map_location=self.config.device)\n","\n","            # 1. Update config from checkpoint\n","            self.config.__dict__.update(checkpoint['config'])\n","            self.data_manager.config.__dict__.update(checkpoint['config'])\n","\n","            # 2. Initialize Tiktoken tokenizer\n","            self.data_manager.initialize_tokenizer()\n","\n","            # 4. Initialize model with updated config\n","            self.model = GPT(self.config).to(self.config.device)\n","\n","            # 5. Load weights with architecture validation\n","            self.model.load_state_dict(checkpoint['model_state_dict'])\n","            self.model.eval()\n","\n","            # 6. Force device alignment\n","            self.data_manager.config.device = self.config.device\n","\n","            # 7. Warmup GPU and verify state\n","            with torch.cuda.amp.autocast():\n","                _ = self.model.generate(\n","                    torch.zeros((1,1), dtype=torch.long, device=self.config.device),\n","                    max_new_tokens=1\n","                )\n","\n","            self._is_initialized = True\n","            print(f\"Model loaded successfully on {self.config.device}\")\n","\n","        except Exception as e:\n","            self.cleanup()\n","            raise RuntimeError(f\"Model loading failed: {str(e)}\")\n","\n","    def generate_text(self, prompt: str, max_tokens: int = 100,\n","                      temperature: float = 0.7, top_k: int = 50) -> str:\n","        \"\"\"Safe text generation with full state validation\"\"\"\n","        self.verify_state()  # Includes tokenizer check\n","\n","        with torch.no_grad(), torch.cuda.amp.autocast():\n","            # Encode the prompt into token IDs and convert to tensor\n","            input_ids = torch.tensor(\n","                self.data_manager.encode(prompt),\n","                dtype=torch.long\n","            ).unsqueeze(0).to(self.config.device)\n","\n","            # Generate new tokens\n","            generated_ids = self.model.generate(\n","                input_ids,\n","                max_new_tokens=max_tokens,\n","                temperature=temperature,\n","                top_k=top_k\n","            )\n","\n","            # Decode the generated token IDs back into text\n","            generated_text = self.data_manager.decode(generated_ids[0].tolist())\n","\n","            return generated_text\n","\n","    def cleanup(self):\n","        \"\"\"Comprehensive cleanup of resources and memory\"\"\"\n","        try:\n","            if hasattr(self, 'model') and self.model is not None:\n","                self.model.cpu()\n","                del self.model\n","                self.model = None\n","\n","            if hasattr(self, 'data_manager') and self.data_manager is not None:\n","                del self.data_manager\n","                self.data_manager = None\n","\n","            if torch.cuda.is_available():\n","                torch.cuda.empty_cache()\n","\n","            gc.collect()\n","\n","            self._is_initialized = False\n","            print(\"Model interface cleaned up successfully\")\n","\n","        except Exception as e:\n","            print(f\"Error during cleanup: {str(e)}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"rZsQkWY0ojx3","executionInfo":{"status":"ok","timestamp":1738355070429,"user_tz":360,"elapsed":1,"user":{"displayName":"Andrew","userId":"07330014611798335629"}}},"outputs":[],"source":["import inspect\n","import torch\n","import gc\n","def run_interactive_chat(model_path: str):\n","    \"\"\"Run interactive chat with the trained model.\"\"\"\n","    try:\n","        print(f\"Loading model from {model_path}...\")\n","        checkpoint = torch.load(model_path, map_location='cuda')\n","\n","        # Get the expected parameters for ModelConfig\n","        valid_config_params = inspect.signature(ModelConfig.__init__).parameters.keys()\n","\n","        # Filter the config dictionary to only include valid parameters\n","        config_dict = {\n","            k: v for k, v in checkpoint['config'].items()\n","            if k in valid_config_params\n","        }\n","\n","        # Create config with only valid parameters\n","        config = ModelConfig(**config_dict)\n","        config.device = 'cuda'  # Force CUDA device\n","\n","        # Initialize model with filtered config\n","        model = GPT(config)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        model.to(config.device)\n","        model.eval()\n","\n","        # Initialize SentencePiece tokenizer\n","        tokenizer = spm.SentencePieceProcessor()\n","        tokenizer.load(config.tokenizer_model_path)\n","\n","        # Set generation parameters\n","        max_new_tokens = 128  # Default value for generation\n","\n","        print(\"\\nModel loaded successfully!\")\n","        print(f\"Vocabulary size: {tokenizer.get_piece_size()}\")\n","        print(\"\\nEnter your prompt (or 'quit' to exit):\")\n","\n","        while True:\n","            prompt = input(\"\\nYou: \")\n","            if prompt.lower() in ['quit', 'exit']:\n","                break\n","\n","            if not prompt:\n","                continue\n","\n","            try:\n","                # Convert input to tensor using SentencePiece\n","                input_ids = torch.tensor([tokenizer.encode_as_ids(prompt)], dtype=torch.long)\n","                input_ids = input_ids.to(config.device)\n","\n","                # Generate response\n","                with torch.no_grad():\n","                    output_ids = model.generate(\n","                        input_ids,\n","                        max_new_tokens=max_new_tokens,\n","                        temperature=0.8,\n","                        top_k=40,\n","                        top_p=0.9\n","                    )\n","\n","                # Convert output tokens to text using SentencePiece\n","                output_text = tokenizer.decode(output_ids[0].tolist())\n","                print(\"\\nModel:\", output_text.strip())\n","\n","            except Exception as e:\n","                print(f\"Error processing input: {str(e)}\")\n","                continue\n","\n","    except Exception as e:\n","        print(f\"Error during inference: {str(e)}\")\n","        print(\"\\nCheckpoint keys:\", checkpoint.keys())\n","        raise\n","\n","    finally:\n","        # Cleanup\n","        torch.cuda.empty_cache()\n","        gc.collect()"]},{"cell_type":"code","source":["'''\n","from dataclasses import dataclass, field\n","from pathlib import Path\n","import sentencepiece as spm\n","import math\n","import os\n","from typing import List, Tuple\n","import random\n","\n","@dataclass\n","class VocabularyOptimizer:\n","    data_path: Path\n","    max_candidates: int = field(default=14, metadata={\"max_value\": 10})\n","    _dataset_stats: dict = field(init=False)\n","\n","    def __post_init__(self):\n","        if not self.data_path.exists():\n","            raise FileNotFoundError(f\"Dataset file not found: {self.data_path}\")\n","        self._dataset_stats = self._analyze_dataset()\n","\n","    def _analyze_dataset(self) -> dict:\n","        with open(self.data_path, 'r', encoding='utf-8') as f:\n","            text = f.read()\n","\n","        return {\n","            'size_mb': os.path.getsize(self.data_path) / (1024 ** 2),\n","            'total_chars': len(text),\n","            'unique_chars': len(set(text)),\n","            'avg_word_length': self._calculate_avg_word_length(text)\n","        }\n","\n","    def _calculate_avg_word_length(self, text: str) -> float:\n","        words = text.split()\n","        return sum(len(word) for word in words) / len(words) if words else 0\n","\n","    def _generate_vocab_candidates(self) -> List[int]:\n","        # Generate candidates from 100 to 20,000\n","        candidates = [100 * (2 ** i) for i in range(14)]  # 100, 200, 400, ..., 1638400\n","        candidates = [c for c in candidates if 100 <= c <= 20000]  # Filter to keep within the range\n","        candidates = sorted(set(candidates))  # Remove duplicates and sort\n","\n","        # Ensure the list does not exceed max_candidates\n","        return candidates[:self.max_candidates]\n","\n","    def _evaluate_vocab_size(self, vocab_size: int) -> Tuple[float, float]:\n","        model_prefix = f\"temp_model_{vocab_size}\"\n","\n","        spm.SentencePieceTrainer.train(\n","            input=str(self.data_path),\n","            model_prefix=model_prefix,\n","            vocab_size=vocab_size,\n","            character_coverage=1.0,\n","            model_type='bpe',\n","            num_threads=os.cpu_count()\n","        )\n","\n","        sp_model = spm.SentencePieceProcessor()\n","        sp_model.load(f\"{model_prefix}.model\")\n","\n","        encoded = sp_model.encode_as_ids(open(self.data_path).read())\n","        entropy = self._calculate_entropy(encoded, vocab_size)\n","        compression_ratio = len(encoded) / self._dataset_stats['total_chars']\n","\n","        os.remove(f\"{model_prefix}.model\")\n","        os.remove(f\"{model_prefix}.vocab\")\n","\n","        return entropy, compression_ratio\n","\n","    def _calculate_entropy(self, token_ids: List[int], vocab_size: int) -> float:\n","        from collections import Counter\n","        counts = Counter(token_ids)\n","        total = len(token_ids)\n","        return -sum((count/total) * math.log2(count/total)\n","                    for count in counts.values() if count > 0)\n","\n","    def optimize_vocab_size(self) -> int:\n","        candidates = self._generate_vocab_candidates()\n","        best_size = None\n","        best_score = float('inf')\n","        results = []\n","\n","        for vocab_size in sorted(candidates):\n","            try:\n","                entropy, compression = self._evaluate_vocab_size(vocab_size)\n","                score = 0.7 * entropy + 0.3 * compression\n","                results.append((vocab_size, score, entropy, compression))\n","\n","                if score < best_score:\n","                    best_score = score\n","                    best_size = vocab_size\n","\n","            except Exception as e:\n","                print(f\"Failed training with vocab_size {vocab_size}: {str(e)}\")\n","                continue\n","\n","        print(\"Vocab Size\\tTotal Score\\tEntropy\\tCompression Ratio\")\n","        for result in results:\n","            print(f\"{result[0]}\\t{result[1]:.4f}\\t{result[2]:.4f}\\t{result[3]:.4f}\")\n","\n","        return best_size if best_size is not None else 8000\n","\n","# Usage example\n","if __name__ == \"__main__\":\n","    dataset_path = Path('/content/drive/MyDrive/LLM/data/training.txt')\n","    optimizer = VocabularyOptimizer(dataset_path)\n","    optimal_vocab_size = optimizer.optimize_vocab_size()\n","    print(f\"Recommended vocabulary size: {optimal_vocab_size}\")\n","    '''"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":92},"id":"4kShVmTxzygL","executionInfo":{"status":"ok","timestamp":1738355070597,"user_tz":360,"elapsed":169,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"57a7a5ca-94be-4097-8c1f-66402f1cbc16"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nfrom dataclasses import dataclass, field\\nfrom pathlib import Path\\nimport sentencepiece as spm\\nimport math\\nimport os\\nfrom typing import List, Tuple\\nimport random\\n\\n@dataclass\\nclass VocabularyOptimizer:\\n    data_path: Path\\n    max_candidates: int = field(default=14, metadata={\"max_value\": 10})\\n    _dataset_stats: dict = field(init=False)\\n\\n    def __post_init__(self):\\n        if not self.data_path.exists():\\n            raise FileNotFoundError(f\"Dataset file not found: {self.data_path}\")\\n        self._dataset_stats = self._analyze_dataset()\\n\\n    def _analyze_dataset(self) -> dict:\\n        with open(self.data_path, \\'r\\', encoding=\\'utf-8\\') as f:\\n            text = f.read()\\n            \\n        return {\\n            \\'size_mb\\': os.path.getsize(self.data_path) / (1024 ** 2),\\n            \\'total_chars\\': len(text),\\n            \\'unique_chars\\': len(set(text)),\\n            \\'avg_word_length\\': self._calculate_avg_word_length(text)\\n        }\\n\\n    def _calculate_avg_word_length(self, text: str) -> float:\\n        words = text.split()\\n        return sum(len(word) for word in words) / len(words) if words else 0\\n\\n    def _generate_vocab_candidates(self) -> List[int]:\\n        # Generate candidates from 100 to 20,000\\n        candidates = [100 * (2 ** i) for i in range(14)]  # 100, 200, 400, ..., 1638400\\n        candidates = [c for c in candidates if 100 <= c <= 20000]  # Filter to keep within the range\\n        candidates = sorted(set(candidates))  # Remove duplicates and sort\\n\\n        # Ensure the list does not exceed max_candidates\\n        return candidates[:self.max_candidates]\\n\\n    def _evaluate_vocab_size(self, vocab_size: int) -> Tuple[float, float]:\\n        model_prefix = f\"temp_model_{vocab_size}\"\\n        \\n        spm.SentencePieceTrainer.train(\\n            input=str(self.data_path),\\n            model_prefix=model_prefix,\\n            vocab_size=vocab_size,\\n            character_coverage=1.0,\\n            model_type=\\'bpe\\',\\n            num_threads=os.cpu_count()\\n        )\\n\\n        sp_model = spm.SentencePieceProcessor()\\n        sp_model.load(f\"{model_prefix}.model\")\\n\\n        encoded = sp_model.encode_as_ids(open(self.data_path).read())\\n        entropy = self._calculate_entropy(encoded, vocab_size)\\n        compression_ratio = len(encoded) / self._dataset_stats[\\'total_chars\\']\\n\\n        os.remove(f\"{model_prefix}.model\")\\n        os.remove(f\"{model_prefix}.vocab\")\\n\\n        return entropy, compression_ratio\\n\\n    def _calculate_entropy(self, token_ids: List[int], vocab_size: int) -> float:\\n        from collections import Counter\\n        counts = Counter(token_ids)\\n        total = len(token_ids)\\n        return -sum((count/total) * math.log2(count/total) \\n                    for count in counts.values() if count > 0)\\n\\n    def optimize_vocab_size(self) -> int:\\n        candidates = self._generate_vocab_candidates()\\n        best_size = None\\n        best_score = float(\\'inf\\')\\n        results = []\\n\\n        for vocab_size in sorted(candidates):\\n            try:\\n                entropy, compression = self._evaluate_vocab_size(vocab_size)\\n                score = 0.7 * entropy + 0.3 * compression\\n                results.append((vocab_size, score, entropy, compression))\\n                \\n                if score < best_score:\\n                    best_score = score\\n                    best_size = vocab_size\\n\\n            except Exception as e:\\n                print(f\"Failed training with vocab_size {vocab_size}: {str(e)}\")\\n                continue\\n\\n        print(\"Vocab Size\\tTotal Score\\tEntropy\\tCompression Ratio\")\\n        for result in results:\\n            print(f\"{result[0]}\\t{result[1]:.4f}\\t{result[2]:.4f}\\t{result[3]:.4f}\")\\n\\n        return best_size if best_size is not None else 8000\\n\\n# Usage example\\nif __name__ == \"__main__\":\\n    dataset_path = Path(\\'/content/drive/MyDrive/LLM/data/training.txt\\')\\n    optimizer = VocabularyOptimizer(dataset_path)\\n    optimal_vocab_size = optimizer.optimize_vocab_size()\\n    print(f\"Recommended vocabulary size: {optimal_vocab_size}\")\\n    '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["'/content/drive/MyDrive/LLM/data/training.txt'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":37},"id":"ymx7ny-h2fut","executionInfo":{"status":"ok","timestamp":1738355070597,"user_tz":360,"elapsed":3,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"acec7e36-5807-4d86-9c0e-421c16c26125"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/MyDrive/LLM/data/training.txt'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":10}]},{"cell_type":"code","execution_count":11,"metadata":{"id":"4wcfsQFa_zBA","colab":{"base_uri":"https://localhost:8080/","height":887},"executionInfo":{"status":"error","timestamp":1738355166731,"user_tz":360,"elapsed":96137,"user":{"displayName":"Andrew","userId":"07330014611798335629"}},"outputId":"9eb6316e-f746-4d68-a8cb-bf65c8d1be7a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model from /content/drive/MyDrive/LLM/checkpoints/best_model_epoch_1_loss_1_6223_20250131_201310.pt...\n","Parameters: 0.62M\n","\n","Model loaded successfully!\n","Vocabulary size: 100\n","\n","Enter your prompt (or 'quit' to exit):\n","\n","You: what if\n","\n","Model: what if you end up at the time. He said, its a good experience. And he was really good for us. They also go from an actual responsible version of Twitter Management and say, b\n","\n","You: what is the name\n","\n","Model: what is the name where I can get out there. We really did they come on anything, Im sure theyre using it with Record. In they go back to the business. I think its going to be anything in revenue strategy and put a billi\n","\n","You: what kind of\n","\n","Model: what kind of millions of everything was actually highlight then up and show notes. Its like it working out there. Theyre very much good. Super important. When you can show up anything abou\n","\n","You: if this is\n","\n","Model: if this is a good value for your friends. Were going to go up being a real future. Yeah. Theyre really not have a little little years ago. And we really know what the value is going to p\n","\n","You: just grow up\n","\n","Model: just grow up on FedMartins is not truly within Pinduoduo is because youre going to share it, you know, you can read the right thing where youre something like yourself, you had the first ti\n","\n","You: what the heck\n","\n","Model: what the heck and stores of products, thats pretty expensive, your hands right. Yeah, I was about CEO of all the chain post. You could see anybody who would have done it, where we talk about\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-e7fdcd20140f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/LLM/checkpoints/best_model_epoch_1_loss_1_6223_20250131_201310.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrun_interactive_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-8-fc50fc02bbc9>\u001b[0m in \u001b[0;36mrun_interactive_chat\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'quit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}],"source":["if __name__ == \"__main__\":\n","    # Configure CUDA for optimal performance\n","    torch.backends.cuda.matmul.allow_tf32 = True\n","    torch.backends.cudnn.allow_tf32 = True\n","    torch.backends.cudnn.benchmark = True\n","\n","    model_path = '/content/drive/MyDrive/LLM/checkpoints/best_model_epoch_1_loss_1_6223_20250131_201310.pt'\n","    run_interactive_chat(model_path)"]},{"cell_type":"markdown","metadata":{"id":"yEDDgwa0gJtQ"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1XOBCh-gyKZcRjkhuVdAjZlAeoBWfCZ47","timestamp":1737833428701},{"file_id":"1zHaOvxWM8twT04dcIaplOJUgz_hCQ1BH","timestamp":1736024230358},{"file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","timestamp":1735856996491}],"mount_file_id":"1SbGTGSSmvh0dH3UwCcmCGFDh_WBTH3a6","authorship_tag":"ABX9TyPDI/noIm7GmuPbNZlinjm2"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}